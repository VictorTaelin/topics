
@inbook{batory_program_2007,
	title = {Program Refactoring, Program Synthesis, and {Model-Driven} Development},
	url = {http://dx.doi.org/10.1007/978-3-540-71229-9_11},
	abstract = {Program refactoring, feature-based and aspect-oriented software synthesis, and model-driven development are disjoint research areas. However, they are all architectural metaprogramming technologies as they treat programs as values and use functions (a.k.a. transformations) to map programs to other programs. In this paper, I explore their underlying connections by reviewing recent advances in each area from an architectural metaprogramming perspective. I conjecture how these areas can converge and outline a theory that may unify them.},
	booktitle = {Compiler Construction},
	author = {Don Batory},
	year = {2007},
	keywords = {calculus,program},
	pages = {156--171},
	comment = {* Conjectures that refactoring, synthesis and model-driven development will converge (but {NOT} "how" as the abstract says) * Review of these areas in the light of the conjecture}
},

@inproceedings{claessen_quickcheck:lightweight_2000,
	title = {{QuickCheck:} a lightweight tool for random testing of Haskell programs},
	isbn = {1-58113-202-6},
	url = {http://portal.acm.org/citation.cfm?id=351240.351266&coll=Portal&dl=GUIDE&CFID=28330491&CFTOKEN=25101450},
	doi = {10.1145/351240.351266},
	abstract = {Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.},
	booktitle = {Proceedings of the fifth {ACM} {SIGPLAN} international conference on Functional programming},
	publisher = {{ACM}},
	author = {Koen Claessen and John Hughes},
	year = {2000},
	pages = {268--279}
},

@techreport{gregor_proposed_2007,
	title = {Proposed {{W}ording} for {{C}oncepts} {({R}evision} 4)},
	author = {D Gregor and B Stroustrup and J Siek and James Widman},
	year = {2007},
	keywords = {sibylle,wgp08},
	comment = {Draft, will be published as a revision to {{\textbackslash}cite{Gregor:2007:PWC}}}
},

@inproceedings{rountev_off-line_2000,
	title = {Off-line variable substitution for scaling points-to analysis},
	booktitle = {{PLDI} '00: Proceedings of the {ACM} {SIGPLAN} 2000 conference on Programming language design and implementation},
	publisher = {{ACM}},
	author = {Atanas Rountev and Satish Chandra},
	year = {2000},
	keywords = {points-to},
	pages = {47--56}
},

@article{plaice_new_1993,
	title = {A new approach to version control},
	volume = {19},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=221137},
	abstract = {A method for controlling versions of software and other hierarchically structured entities is presented. Using the variant structure principle, a particular version of an entire system is formed by combining the most relevant existing versions of the various components of the system. An algebraic version language that allows histories (numbered series), subversions (or variants), and joins is described. It is shown that the join operation is simply the lattice least upper bound and together with the variant structure principle, provides a systematic framework for recombining divergent variants. The utility of this approach is demonstrated using {LEMUR,} a programming environment for modular C programs, which was developed using itself. The ways in which this notion of versions is related to the possible world semantics of intensional logic are discussed},
	number = {3},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {J Plaice and {WW} Wadge},
	year = {1993},
	keywords = {cm,vc},
	pages = {268--276},
	comment = {ignores the problem of merging}
},

@inproceedings{ager_functional_2003,
	title = {A functional correspondence between evaluators and abstract machines},
	isbn = {1581137052},
	url = {http://dx.doi.org/10.1145/888251.888254},
	booktitle = {{PPDP} '03: Proceedings of the 5th {ACM} {SIGPLAN} international conference on Principles and practice of declaritive programming},
	publisher = {{ACM} Press},
	author = {Mads Ager and Dariusz Biernacki and Olivier Danvy and Jan Midtgaard},
	year = {2003},
	keywords = {cps},
	pages = {8--19}
},

@inproceedings{wadler_theorems_1989,
	address = {Imperial College, London, United Kingdom},
	title = {Theorems for free!},
	isbn = {0-89791-328-0},
	url = {http://portal.acm.org/citation.cfm?id=99404},
	doi = {10.1145/99370.99404},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the fourth international conference on Functional programming languages and computer architecture},
	publisher = {{ACM}},
	author = {Philip Wadler},
	year = {1989},
	pages = {347--359}
},

@book{bird_algebra_1997,
	title = {Algebra of programming},
	isbn = {{013507245X}},
	url = {http://portal.acm.org/citation.cfm?id=248932},
	publisher = {{Prentice-Hall,} Inc.},
	author = {Richard Bird and Oege de Moor},
	year = {1997},
	keywords = {aop}
},

@article{hughes_functional_1989,
	title = {Why Functional Programming Matters},
	volume = {32},
	url = {http://citeseer.ist.psu.edu/hughes84why.html},
	abstract = {As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised. Functional languages push those limits back. In this paper we show that two features of functional languages in particular, higher-order functions and lazy...},
	number = {2},
	journal = {Computer Journal},
	author = {J Hughes},
	year = {1989},
	keywords = {haskell},
	pages = {98--107}
},

@inproceedings{chakravarty_associated_2005,
	title = {Associated types with class},
	isbn = {{158113830X}},
	url = {http://dx.doi.org/10.1145/1040305.1040306},
	booktitle = {{POPL} '05: Proceedings of the 32nd {ACM} {SIGPLAN-SIGACT} sysposium on Principles of programming languages},
	publisher = {{ACM} Press},
	author = {Manuel Chakravarty and Gabriele Keller and Simon Peyton Jones and Simon Marlow},
	year = {2005},
	keywords = {typeclass},
	pages = {1--13}
},

@article{apiwattanapong_jdiff:differencing_2007,
	title = {{JDiff:} A differencing technique and tool for object-oriented programs},
	volume = {14},
	issn = {0928-8910},
	url = {http://dx.doi.org/10.1007/s10515-006-0002-0},
	number = {1},
	journal = {Automated Software Engineering},
	author = {Apiwattanapong and Taweesup and Orso and Alessandro and Harrold and Mary},
	month = mar,
	year = {2007},
	keywords = {diff},
	pages = {3--36}
},

@inproceedings{sulzmann_modular_2006,
	title = {Modular generic programming with extensible superclasses},
	isbn = {1595934926},
	url = {http://dx.doi.org/10.1145/1159861.1159869},
	booktitle = {{WGP} '06: Proceedings of the 2006 {ACM} {SIGPLAN} workshop on Generic programming},
	publisher = {{ACM}},
	author = {Martin Sulzmann and Meng Wang},
	year = {2006},
	keywords = {open,typeclass},
	pages = {55--65}
},

@inproceedings{lmmel_expression_2008,
	title = {{{The} expression lemma}},
	booktitle = {{{Proceedings} of Mathematics of Program Construction {(MPC)} 2008}},
	publisher = {Springer},
	author = {Ralf L\"{a}mmel and Ondrej Rypacek},
	year = {2008},
	keywords = {open},
	comment = {To appear}
},

@article{swierstra_data_2008,
	title = {Data types \`{a} la carte},
	volume = {Forthcoming},
	url = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=1813324},
	abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell's monolithic {IO} monad.},
	number = {-1},
	journal = {Journal of Functional Programming},
	author = {Wouter Swierstra},
	year = {2008},
	keywords = {aop,open},
	pages = {1--14}
},

@inproceedings{carlsson_monads_2002,
	address = {Pittsburgh, {PA,} {USA}},
	title = {Monads for incremental computing},
	isbn = {1-58113-487-8},
	url = {http://portal.acm.org/citation.cfm?id=581482},
	doi = {10.1145/581478.581482},
	abstract = {This paper presents a monadic approach to incremental computation, suitable for purely functional languages such as Haskell. A program that uses incremental computation is able to perform an incremental amount of computation to accommodate for changes in input data. Recently, Acar, Blelloch and Harper presented a small Standard {ML} library that supports efficient, high-level incremental computations [1]. Here, we present a monadic variant of that library, written in Haskell extended with first-class references. By using monads, not only are we able to provide a purely functional interface to the library, the types also enforce "correct usage" without having to resort to any type-system extension. We also find optimization opportunities based on standard monadic {combinators.This} is an exercise in putting to work monad transformers with environments, references, and continuations.},
	booktitle = {Proceedings of the seventh {ACM} {SIGPLAN} international conference on Functional programming},
	publisher = {{ACM}},
	author = {Magnus Carlsson},
	year = {2002},
	pages = {26--35}
},

@book{vandevoorde_c++_2002,
	title = {C++ Templates: The Complete Guide},
	isbn = {0201734842},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&path=ASIN/0201734842},
	publisher = {{{Addison-Wesley} Professional}},
	author = {David Vandevoorde and Nicolai Josuttis},
	month = nov,
	year = {2002},
	keywords = {cpp,templates,wgp08}
},

@misc{_representing_????,
	title = {Representing control: a study of the {CPS} transformation},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.84},
	abstract = {This paper investigates the transformation of v-terms into continuation-passing style {(CPS).} We show that by appropriate j-expansion of Fischer and Plotkin's two-pass equational specification of the {CPS} transform, we can obtain a static and context-free separation of the result terms into \&quot;essential \&quot; and \&quot;administrative \&quot; constructs. Interpreting the former as syntax builders and the latter as directly executable code, we obtain a simple and efficient one-pass transformation algorithm, easily extended to conditional expressions, recursive definitions, and similar constructs. This new transformation algorithm leads to a simpler proof of Plotkin's simulation and indifference results. We go on to show how {CPS-based} control operators similar to, but more general than, Scheme's call/cc can be naturally accommodated by the new transformation algorithm. To demonstrate the expressive power of these operators, we use them to present an equivalent but even more concise formulation of the efficient {CPS} transformation algorithm. Finally, we relate the fundamental ideas underlying this derivation to similar concepts from other works on program manipulation; we derive a one-pass {CPS} transformation of n-terms; and we outline some promising areas for future research.},
	keywords = {cps},
	howpublished = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.84}
},

@inproceedings{lh_open_2006,
	address = {Venice, Italy},
	title = {Open data types and open functions},
	isbn = {1-59593-388-3},
	url = {http://portal.acm.org/citation.cfm?id=1140352},
	doi = {10.1145/1140335.1140352},
	abstract = {The problem of supporting the modular extensibility of both data and functions in one programming language at the same time is known as the expression problem. Functional languages traditionally make it easy to add new functions, but extending data (adding new data constructors) requires modifying existing code. We present a semantically and syntactically lightweight variant of open data types and open functions as a solution to the expression problem in the Haskell language. Constructors of open data types and equations of open functions may appear scattered throughout a program with several modules. The intended semantics is as follows: the program should behave as if the data types and functions were closed, defined in one place. The order of function equations is determined by best-fit pattern matching, where a specific pattern takes precedence over an unspecific one. We show that our solution is applicable to the expression problem, generic programming, and exceptions. We sketch two implementations: a direct implementation of the semantics, and a scheme based on mutually recursive modules that permits separate compilation},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} international conference on Principles and practice of declarative programming},
	publisher = {{ACM}},
	author = {Andres L\"{o}h and Ralf Hinze},
	year = {2006},
	keywords = {expression problem,extensible data types,extensible exceptions,extensible functions,functional programming,generic programming,mutually recursive modules,open},
	pages = {133--144}
},

@inbook{bierman_upgradej:_2008,
	title = {{UpgradeJ:} Incremental Typechecking for Class Upgrades},
	url = {http://dx.doi.org/10.1007/978-3-540-70592-5_11},
	abstract = {One of the problems facing developers is the constant evolution of components that are used to build applications. This evolution is typical of any multi-person or multi-site software project. How can we program in this environment? More precisely, how can language design address such evolution? In this paper we attack two significant issues that arise from constant component evolution: we propose language-level extensions that permit multiple, co-existing versions of classes and the ability to dynamically upgrade from one version of a class to another, whilst still maintaining type safety guarantees and requiring only lightweight extensions to the runtime infrastructure. We show how our extensions, whilst intuitive, provide a great deal of power by giving a number of examples. Given the subtlety of the problem, we formalize a core fragment of our language and prove a number of important safety properties.},
	booktitle = {{ECOOP} 2008 \^{a}?? {Object-Oriented} Programming},
	author = {Gavin Bierman and Matthew Parkinson and James Noble},
	year = {2008},
	keywords = {evolution},
	pages = {235--259}
},

@article{jones_dictionary-free_1994,
	title = {{Dictionary-Free} Overloading by Partial Evaluation},
	volume = {8},
	url = {http://dx.doi.org/10.1007/BF01019005},
	number = {3},
	journal = {{LISP} and Symbolic Computation},
	author = {Mark Jones},
	year = {1994},
	keywords = {sibylle,typeclass,wgp08},
	pages = {229--248}
},

@techreport{gregor_concepts_2006,
	title = {{{C}oncepts} for the {{{\textbackslash}{\textbackslash}Cpp0x}} Standard Library: Containers},
	author = {D Gregor},
	year = {2006},
	keywords = {sibylle,wgp08}
},

@article{claessen_parallel_2004,
	title = {Parallel Parsing Processes},
	volume = {14},
	number = {6},
	journal = {Journal of Functional Programming},
	author = {Koen Claessen},
	year = {2004},
	pages = {741--757}
},

@article{fokkinga_datatype_1996,
	title = {Datatype Laws Without Signatures},
	volume = {6},
	url = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=4131224},
	doi = {{10.1017/S0960129500000852}},
	number = {01},
	journal = {Mathematical Structures in Computer Science},
	author = {Maarten M. Fokkinga},
	year = {1996},
	pages = {1--32}
},

@inproceedings{fegaras_revisiting_1996,
	address = {St. Petersburg Beach, Florida, United States},
	title = {Revisiting catamorphisms over datatypes with embedded functions (or, programs from outer space)},
	isbn = {0-89791-769-3},
	url = {http://portal.acm.org/citation.cfm?id=237792},
	doi = {10.1145/237721.237792},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the 23rd {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Leonidas Fegaras and Tim Sheard},
	year = {1996},
	pages = {284--294}
},

@inproceedings{pennings_using_1992,
	title = {Using Cached Functions and Constructors for Incremental Attribute Evaluation},
	isbn = {3-540-55844-6},
	url = {http://portal.acm.org/citation.cfm?id=646448.756843&coll=ACM&dl=ACM&CFID=13026192&CFTOKEN=34892732},
	booktitle = {Proceedings of the 4th International Symposium on Programming Language Implementation and Logic Programming},
	publisher = {{Springer-Verlag}},
	author = {Maarten Pennings and S. Doaitse Swierstra and Harald Vogt},
	year = {1992},
	pages = {130--144}
},

@misc{schrijvers_type_????,
	title = {Type Checking with Open Type Functions},
	author = {Tom Schrijvers and Simon Peyton Jones and Manuel Chakravarty and Martin Sulzmann},
	keywords = {typeclass}
},

@misc{lh_principled_????,
	title = {A Principled Approach to Version Control},
	author = {Andres L\"{o}h and Wouter Swierstra and Daan Leijen},
	keywords = {vc,vc-project}
},

@misc{kiselyov_polymorphic_2006,
	title = {Polymorphic variants: solving the expression problem},
	url = {http://okmij.org/ftp/Haskell/generics.html#PolyVariant},
	author = {Oleg Kiselyov},
	month = jul,
	year = {2006},
	keywords = {open},
	howpublished = {{http://okmij.org/ftp/Haskell/generics.html\#PolyVariant}}
},

@inbook{malik_generating_2007,
	title = {Generating Representation Invariants of Structurally Complex Data},
	url = {http://dx.doi.org/10.1007/978-3-540-71209-1_5},
	abstract = {Generating likely invariants using dynamic analyses is becoming an increasingly effective technique in software checking methodologies. This paper presents Deryaft, a novel algorithm for generating likely representation invariants of structurally complex data. Given a small set of concrete structures, Deryaft analyzes their key characteristics to formulate local and global properties that the structures exhibit. For effective formulation of structural invariants, Deryaft focuses on graph properties, including reachability, and views the program heap as an edge-labeled graph. Deryaft outputs a Java predicate that represents the invariants; the predicate takes an input structure and returns true if and only if it satisfies the invariants. The invariants generated by Deryaft directly enable automation of various existing frameworks, such as the Korat test generation framework and the Juzi data structure repair framework, which otherwise require the user to provide the invariants. Experimental results with the Deryaft prototype show that it feasibly generates invariants for a range of subject structures, including libraries as well as a stand-alone application.},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	author = {Muhammad Malik and Aman Pervaiz and Sarfraz Khurshid},
	year = {2007},
	keywords = {invariant,recovery},
	pages = {34--49},
	comment = {* works on a snapshot of dynamic data * graph-like representation of heap * popular contender: Daikon (can be better for some inputs)}
},

@article{ghezzi_incremental_1979,
	title = {Incremental Parsing},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=357062.357066},
	doi = {10.1145/357062.357066},
	abstract = {An incremental parser is a device which is able to perform syntax analysis in an incremental way, avoiding complete reparsing of a program after each modification. The incremental parser presented extends the conventional {LR} parsing algorithm and its performance is compared with that of a conventional parser. Suggestions for an implementation and possible extensions to other parsing methods are also discussed.},
	number = {1},
	journal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Carlo Ghezzi and Dino Mandrioli},
	year = {1979},
	pages = {58--70}
},

@article{boudol_semantics_2000,
	title = {On the semantics of the call-by-name {CPS} transform},
	volume = {234},
	url = {http://portal.acm.org/citation.cfm?id=331681},
	number = {1-2},
	journal = {Theor. Comput. Sci.},
	author = {G\'{e}rard Boudol},
	year = {2000},
	keywords = {b\"{o}hm-out technique,continuation-passing-style transforms,l\'{e}vy-longo trees,\&lgr;-calculus},
	pages = {309--321}
},

@inbook{glck_derivation_2004,
	title = {Derivation of Deterministic Inverse Programs Based on {LR} Parsing},
	url = {http://www.springerlink.com/content/tfaxx9x8mxukcreb },
	abstract = {We present a method for automatic program inversion of functional programs based on methods of {LR} parsing. We formalize the transformation and illustrate it with the inversion of a program for run-length encoding. We solve one of the main problems of automatic program inversion\^{a}??the elimination of nondeterminism\^{a}??by viewing an inverse program as a context-free grammar and applying to it methods of {LR} parsing to turn it into a recursive, deterministic inverse program. This improves the efficiency of the inverse programs and greatly expands the application range of our earlier method for program inversion.},
	booktitle = {Functional and Logic Programming},
	author = {Robert Gl\"{u}ck and Masahiko Kawabe},
	year = {2004},
	keywords = {aop},
	pages = {291--306}
},

@inproceedings{swierstra_fast_1999,
	title = {Fast, Error Correcting Parser Combinators: A Short Tutorial},
	isbn = {{3-540-66694-X}},
	url = {http://portal.acm.org/citation.cfm?id=647009.712536&coll=ACM&dl=ACM&CFID=13026192&CFTOKEN=34892732},
	booktitle = {Proceedings of the 26th Conference on Current Trends in Theory and Practice of Informatics on Theory and Practice of Informatics},
	publisher = {{Springer-Verlag}},
	author = {S. Doaitse Swierstra and Pablo R. Azero Alcocer},
	year = {1999},
	pages = {112--131}
},

@article{svenningsson_shortcut_2002,
	title = {Shortcut fusion for accumulating parameters {\textbackslash}\& zip-like functions},
	volume = {37},
	url = {http://portal.acm.org/citation.cfm?id=581491&dl=GUIDE&coll=GUIDE&CFID=36359476&CFTOKEN=55918602},
	doi = {10.1145/583852.581491},
	abstract = {We present an alternative approach to shortcut fusion based on the function unfoldr,. Despite its simplicity the technique can remove intermediate lists in examples which are known to be difficult. We show that it can remove all lists from definitions involving zip-like functions and functions using accumulating parameters.},
	number = {9},
	journal = {{SIGPLAN} Not.},
	author = {Josef Svenningsson},
	year = {2002},
	keywords = {deforestation,functional programming,intermediate data structures,optimisation,program transformation},
	pages = {124--132}
},

@inbook{jones_type_2000,
	title = {Type Classes with Functional Dependencies},
	url = {http://dx.doi.org/10.1007/3-540-46425-5_15},
	abstract = {Type classes in Haskell allow programmers to define functions that can be used on a set of different types, with a potentially different implementation in each case. For example, type classes are used to support equality and numeric types, and for monadic programming. A commonly requested extension to support \^{a}??multiple parameters\^{a}?? allows a more general interpretation of classes as relations on types, and has many potentially useful applications. Unfortunately, many of these examples do not work well in practice, leading to ambiguities and inaccuracies in inferred types and delaying the detection of type errors. This paper illustrates the kind of problems that can occur with multiple parameter type classes, and explains how they can be resolved by allowing programmers to specify explicit dependencies between the parameters. A particular novelty of this paper is the application of ideas from the theory of relational databases to the design of type systems.},
	booktitle = {Programming Languages and Systems},
	author = {Mark Jones},
	year = {2000},
	keywords = {typeclass},
	pages = {230--244}
},

@book{knuth_art_1998,
	edition = {2},
	title = {The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)},
	isbn = {0201896850},
	publisher = {{Addison-Wesley} Professional},
	author = {Donald E. Knuth},
	month = may,
	year = {1998}
},

@article{wilcox_design_1976,
	title = {The design and implementation of a table driven, interactive diagnostic programming system},
	volume = {19},
	url = {http://portal.acm.org/citation.cfm?id=360363.360367},
	doi = {10.1145/360363.360367},
	abstract = {{CAPS} is a highly interactive diagnostic compiler/interpreter that allows beginning programmers to prepare, debug, and execute fairly simple programs at a graphics display terminal. Complete syntax checking and most semantic analysis is performed as the program is entered and as it is subsequently edited. Analysis is performed character by character. The most remarkable feature of {CAPS} is its ability to automatically diagnose errors both at compile time and at run time. Errors are not automatically corrected. Instead, {CAPS} interacts with the student to help him find the cause of his error. Most components of {CAPS} are table driven, both to reduce the space needed for implementation and to increase the flexibility of the multilingual system. Over 500 students have used {CAPS} to learn Fortran, {PL/I,} or Cobol in conjunction with a computer assisted course on introductory computer science.},
	number = {11},
	journal = {Commun. {ACM}},
	author = {Thomas R. Wilcox and Alan M. Davis and Michael H. Tindall},
	year = {1976},
	keywords = {computer assisted instruction,computer science education,debugging,error correction,interactive programming,interpreters,table driven compilers},
	pages = {609--616}
},

@misc{asklund_identifying_1994,
	title = {Identifying conflicts during structural merge},
	url = {http://citeseer.ist.psu.edu/asklund94identifying.html},
	abstract = {. This paper presents a model for controlling the evolution of documents concurrently developed by teams of authors. Optimistic check-out of revisions and alternatives, and hierarchic merge making use of default rules is presented. In particular the different situations occurring during a merge of parallel development lines and the benefit of storing the full evolution history is discussed. 1 Introduction To cooperate have always been hard. Now when the cooperating persons may be spread all...},
	author = {U Asklund},
	year = {1994},
	keywords = {vc},
	comment = {older version of other paper by Asklund}
},

@article{buckley_towardstaxonomy_2005,
	title = {Towards a taxonomy of software change},
	volume = {17},
	url = {http://dx.doi.org/10.1002/smr.319},
	abstract = {Previous taxonomies of software change have focused on the purpose of the change (i.e., the why) rather than the underlying mechanisms. This paper proposes a taxonomy of software change based on characterizing the mechanisms of change and the factors that influence these mechanisms. The ultimate goal of this taxonomy is to provide a framework that positions concrete tools, formalisms and methods within the domain of software evolution. Such a framework would considerably ease comparison between the various mechanisms of change. It would also allow practitioners to identify and evaluate the relevant tools, methods and formalisms for a particular change scenario. As an initial step towards this taxonomy, the paper presents a framework that can be used to characterize software change support tools and to identify the factors that impact on the use of these tools. The framework is evaluated by applying it to three different change support tools and by comparing these tools based on this analysis. Copyright \^{A}{\textcopyright} 2005 John Wiley \& Sons, Ltd.},
	number = {5},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Jim Buckley and Tom Mens and Matthias Zenger and Awais Rashid and G\~{A}{\textonequarter}nter Kniesel},
	year = {2005},
	keywords = {evolution},
	pages = {309--332}
},

@article{ager_functional_2004,
	title = {A functional correspondence between call-by-need evaluators and lazy abstract machines},
	volume = {90},
	url = {http://dx.doi.org/10.1016/j.ipl.2004.02.012},
	abstract = {We bridge the gap between compositional evaluators and abstract machines for the lambda-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization of continuations. This article is a followup of our article at {PPDP} 2003, where we consider call by name and call by value. Here, however, we consider call by need. We derive a lazy abstract machine from an ordinary call-by-need evaluator that threads a heap of updatable cells. In this resulting abstract machine, the continuation fragment for updating a heap cell naturally appears as an [`]update marker', an implementation technique that was invented for the Three Instruction Machine and subsequently used to construct lazy variants of Krivine's abstract machine. Tuning the evaluator leads to other implementation techniques such as unboxed values. The correctness of the resulting abstract machines is a corollary of the correctness of the original evaluators and of the program transformations used in the derivation.},
	number = {5},
	journal = {Information Processing Letters},
	author = {Mads Ager and Olivier Danvy and Jan Midtgaard},
	month = jun,
	year = {2004},
	keywords = {cps,defunc},
	pages = {223--232}
},

@article{mcbride_applicative_2007,
	title = {Applicative programming with effects},
	volume = {18},
	url = {http://dx.doi.org/10.1017/S0956796807006326},
	abstract = {In this article, we introduce Applicative functors \^{a}?? an abstract characterisation of an applicative style of effectful programming, weaker than Monads and hence more widespread. Indeed, it is the ubiquity of this programming pattern that drew us to the abstraction. We retrace our steps in this article, introducing the applicative pattern by diverse examples, then abstracting it to define the Applicative type class and introducing a bracket notation that interprets the normal application syntax in the idiom of an Applicative functor. Furthermore, we develop the properties of applicative functors and the generic operations they support. We close by identifying the categorical structure of applicative functors and examining their relationship both with Monads and with Arrow.},
	number = {01},
	journal = {Journal of Functional Programming},
	author = {Conor {McBride} and {ROSS} Paterson},
	year = {2007},
	pages = {1--13}
},

@inbook{robbes_approach_2007,
	title = {An Approach to Software Evolution Based on Semantic Change},
	url = {http://dx.doi.org/10.1007/978-3-540-71289-3_4},
	abstract = {The analysis of the evolution of software systems is a useful source of information for a variety of activities, such as reverse engineering, maintenance, and predicting the future evolution of these systems. Current software evolution research is mainly based on the information contained in versioning systems such as {CVS} and {SubVersion.} But the evolutionary information contained therein is incomplete and of low quality, hence limiting the scope of evolution research. It is incomplete because the historical information is only recorded at the explicit request of the developers (a commit in the classical checkin/checkout model). It is of low quality because the file-based nature of versioning systems leads to a view of software as being a set of files. In this paper we present a novel approach to software evolution analysis which is based on the recording of all semantic changes performed on a system, such as refactorings. We describe our approach in detail, and demonstrate how it can be used to perform fine-grained software evolution analysis.},
	booktitle = {Fundamental Approaches to Software Engineering},
	author = {Romain Robbes and Michele Lanza and Mircea Lungu},
	year = {2007},
	keywords = {evolution},
	pages = {27--41}
},

@article{okasaki_call-by-need_1994,
	title = {Call-by-need and continuation-passing style},
	volume = {7},
	url = {http://dx.doi.org/10.1007/BF01019945},
	doi = {{10.1007/BF01019945}},
	abstract = {This paper examines the transformation of call-by-need ? terms into continuation-passing style {(CPS).} It begins by presenting a simple transformation of call-by-need ? terms into program graphs and a reducer for such graphs. From this, an informal derivation is carried out, resulting in a translation from ? terms into self-reducing program graphs, where the graphs are represented as {CPS} terms involving storage operations. Though informal, the derivation proceeds in simple steps, and the resulting translation is taken to be our canonical {CPS} transformation for call-by-need ? terms.},
	number = {1},
	journal = {{LISP} and Symbolic Computation},
	author = {Chris Okasaki and Peter Lee and David Tarditi},
	year = {1994},
	pages = {57--81}
},

@inproceedings{altenkirch_generic_2003,
	title = {Generic Programming within Dependently Typed Programming},
	isbn = {1402073747},
	url = {http://portal.acm.org/citation.cfm?id=647100.717294},
	booktitle = {Proceedings of the {IFIP} {TC2/WG2.1} Working Conference on Generic Programming},
	publisher = {Kluwer, {B.V.}},
	author = {Thorsten Altenkirch and Conor Mcbride},
	year = {2003},
	keywords = {aop,generic},
	pages = {1--20}
},

@inproceedings{mitchell_deriving_2007,
	title = {Deriving Generic Functions by Example},
	url = {\erb'http://www-users.cs.york.ac.uk/~ndm/downloads/paper-deriving_generic_functions_by_example-26_oct_2007.pdf'},
	publisher = {Tech. Report {YCS-2007-421,} Dept. of Computer Science, University of York, {UK}},
	author = {Neil Mitchell and Jan M\~{A}{\textonequarter}hlberg and Juan Perna},
	year = {2007},
	keywords = {polytypic},
	pages = {55--62}
},

@inproceedings{xu_static_2009,
	address = {Savannah, {GA,} {USA}},
	title = {Static contract checking for Haskell},
	isbn = {978-1-60558-379-2},
	url = {http://portal.acm.org/citation.cfm?id=1480881.1480889},
	doi = {10.1145/1480881.1480889},
	abstract = {Program errors are hard to detect and are costly both to programmers who spend significant efforts in debugging, and for systems that are guarded by runtime checks. Static verification techniques have been applied to imperative and object-oriented languages, like Java and C\#, but few have been applied to a higher-order lazy functional language, like Haskell. In this paper, we describe a sound and automatic static verification framework for Haskell, that is based on contracts and symbolic execution. Our approach is modular and gives precise blame assignments at compile-time in the presence of higher-order functions and laziness.},
	booktitle = {Proceedings of the 36th annual {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Dana N. Xu and Simon Peyton Jones and Koen Claessen},
	year = {2009},
	keywords = {contract satisfaction,static contract checking},
	pages = {41--52}
},

@inproceedings{zeller_yesterday_1999,
	title = {Yesterday, my program worked. Today, it does not. Why?},
	isbn = {0163-5948},
	url = {http://dx.doi.org/10.1145/318773.318946},
	booktitle = {{ESEC/FSE-7:} Proceedings of the 7th European software engineering conference held jointly with the 7th {ACM} {SIGSOFT} international symposium on Foundations of software engineering},
	publisher = {{Springer-Verlag}},
	author = {Andreas Zeller},
	year = {1999},
	keywords = {dd},
	pages = {253--267},
	comment = {How to find a minimal set of changes that introduced a bug. Various heuristics to avoid testing inconsistent subsets of changes.}
},

@misc{mens_towardstaxonomy_2003,
	title = {Towards a Taxonomy of Software Evolution},
	url = {http://citeseer.ist.psu.edu/601028.html},
	abstract = {Previous taxonomies of software evolution have focused on the purpose of the change (i.e., the why) rather than the underlying mechanisms. This paper proposes a taxonomy of software evolution based on the characterizing mechanisms of change and the factors that influence these mechanisms. The taxonomy is organized into the following logical groupings: temporal properties, objects of change, system properties, and change support.},
	author = {T Mens and J Buckley and M Zenger and A Rashid},
	year = {2003},
	keywords = {evolution}
},

@inproceedings{hunt_distributed_1997,
	title = {Distributed Configuration Management via Java and the World Wide Web},
	url = {http://citeseer.ist.psu.edu/hunt97distributed.html},
	abstract = {. The introduction of Java has been heralded as a revolution in network computing. Certainly, machine and operating system independent applets flittering through the Internet promised to jazz up web surfing; but could they be used to advantage for distributed computing ? The authors had encountered substantial problems in implementing a distributed revision control system, called {WWRC,} based on passive Web browsers. Java seemed to offer solutions to these problems. To this end, the...},
	booktitle = {System Configuration Management},
	author = {James Hunt and Frank Lamers and Jurgen Reuter and Walter Tichy},
	year = {1997},
	keywords = {vc},
	pages = {161--174}
},

@inproceedings{magnusson_fine_1996,
	title = {Fine Grained Version Control of Configurations in {COOP/Orm}},
	url = {http://citeseer.ist.psu.edu/19322.html},
	abstract = {. This paper describes a unified approach to version control of documents and configurations. Hierarchical structure, which is present in most documents such as programs, is recognized and utilized in a fine-grained version control system. The same mechanism is used for version control of configurations and extended to handle {DAGs} as well as trees. Change propagation within one hierarchical document is automatic while bindings between documents are explicit. The model is novel because of its...},
	booktitle = {System Configuration Management},
	author = {Boris Magnusson and Ulf Asklund},
	year = {1996},
	keywords = {vc},
	pages = {31--48},
	comment = {blabla}
},

@misc{khanna_formal_2006,
	title = {A Formal Investigation of Diff3},
	url = {http://www.cis.upenn.edu/~bcpierce/papers/diff3-short.pdf},
	author = {Sanjeev Khanna and Keshav Kunal and Benjamin Pierce},
	year = {2006},
	keywords = {bibtex-import,diff3,vc,vc-project},
	howpublished = {http://www.cis.upenn.edu/{\textasciitilde}bcpierce/papers/diff3-short.pdf},
	comment = {{(private-note)Manuscript}}
},

@inproceedings{dart_concepts_1991,
	title = {Concepts in Configuration Management Systems},
	url = {http://citeseer.ist.psu.edu/dart90concepts.html},
	abstract = {: There has been considerable progress con- 1.1 Definition of Configuration Management cerning support for software configuration management Software {CM} is a discipline for controlling the evolution {(CM)} in environments and tools. This paper's intent is to of software systems. Classic discussions about {CM} are highlight the user concepts provided by existing {CM} sys- given in texts such as [3] and [4]. A standard definition tems. These are shown as a spectrum. In the spectrum, taken from {IEEE...}},
	booktitle = {Proceedings of the 3rd International Workshop on Software Configuration Management},
	author = {Susan Dart and Peter Feiler},
	year = {1991},
	keywords = {vc},
	pages = {1--18}
},

@misc{gregor_conceptgcc_2008,
	title = {{Concept{GCC}} --- a prototype compiler for {{\textbackslash}{\textbackslash}Cpp{}} concepts},
	author = {Douglas Gregor},
	year = {2008},
	keywords = {sibylle,wgp08}
},

@inbook{abbott_categories_2003,
	title = {Categories of Containers},
	url = {http://dx.doi.org/10.1007/3-540-36576-1_2},
	abstract = {We introduce the notion of containers as a mathematical formalisation of the idea that many important datatypes consist of
templates where data is stored. We show that containers have good closure properties under a variety of constructions including
the formation of initial algebras and final coalgebras. We also show that containers include strictly positive types and shapely
types but that there are containers which do not correspond to either of these. Further, we derive a representation result
classifying the nature of polymorphic functions between containers. We finish this paper with an application to the theory
of shapely types and refer to a forthcoming paper which applies this theory to differentiable types.
},
	booktitle = {Foundations of Software Science and Computation Structures},
	author = {Michael Abbott and Thorsten Altenkirch and Neil Ghani},
	year = {2003},
	pages = {23--38}
},

@article{mens_state-of--art_2002,
	title = {A state-of-the-art survey on software merging},
	volume = {28},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1000449},
	abstract = {Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions},
	number = {5},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {T Mens},
	year = {2002},
	keywords = {survey,vc,vc-project},
	pages = {449--462}
},

@article{hinze_generics_2004,
	title = {Generics for the masses},
	volume = {39},
	url = {http://portal.acm.org/citation.cfm?id=1016882},
	doi = {10.1145/1016848.1016882},
	abstract = {A generic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of generic functions are the functions that can be derived in Haskell, such as show, read, and '=='. The recent years have seen a number of proposals that support the definition of generic functions. Some of the proposals define new languages, some define extensions to existing languages. As a common characteristic none of the proposals can be made to work within Haskell 98: they all require something extra, either a more sophisticated type system or an additional language construct. The purpose of this pearl is to show that one can, in fact, program generically within Haskell 98 obviating to some extent the need for fancy type systems or separate tools. Haskell's type classes are at the heart of this approach: they ensure that generic functions can be defined succinctly and, in particular, that they can be used painlessly.},
	number = {9},
	journal = {{SIGPLAN} Not.},
	author = {Ralf Hinze},
	year = {2004},
	keywords = {generic programming,haskell 98,type classes},
	pages = {236--243}
},

@article{hudson_incremental_1991,
	title = {Incremental attribute evaluation: a flexible algorithm for lazy update},
	volume = {13},
	issn = {0164-0925},
	url = {http://dx.doi.org/10.1145/117009.117012},
	number = {3},
	journal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Scott Hudson},
	month = jul,
	year = {1991},
	keywords = {lazy},
	pages = {315--341}
},

@inproceedings{voigtlnder_proving_2008,
	address = {San Francisco, California, {USA}},
	title = {Proving correctness via free theorems: the case of the destroy/build-rule},
	isbn = {978-1-59593-977-7},
	url = {http://portal.acm.org/citation.cfm?id=1328408.1328412},
	doi = {10.1145/1328408.1328412},
	abstract = {Free theorems feature prominently in the field of program transformation for pure functional languages such as Haskell. However, somewhat disappointingly, the semantic properties of so based transformations are often established only very superficially. This paper is intended as a case study showing how to use the existing theoretical foundations and formal methods for improving the situation. To that end, we investigate the correctness issue for a new transformation rule in the short cut fusion family. This destroy/build-rule provides a certain reconciliation between the competing foldr/build- and destroy/unfoldr-approaches to eliminating intermediate lists. Our emphasis is on systematically and rigorously developing the rule's correctness proof, even while paying attention to semantic aspects like potential nontermination and mixed strict/nonstrict evaluation.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGPLAN} symposium on Partial evaluation and semantics-based program manipulation},
	publisher = {{ACM}},
	author = {Janis Voigtl\"{a}nder},
	year = {2008},
	keywords = {correctness proofs,intermediate data structures,program transformations,rank-2 types,relational parametricity,shortcut deforestation,theorems for free},
	pages = {13--20}
},

@inproceedings{elliott_functional_1997,
	address = {Amsterdam, The Netherlands},
	title = {Functional reactive animation},
	isbn = {0-89791-918-1},
	url = {http://portal.acm.org/citation.cfm?id=258948.258973&type=series},
	doi = {10.1145/258948.258973},
	abstract = {Fran {(Functional} Reactive Animation) is a collection of data types and functions for composing richly interactive, multimedia animations. The key ideas in Fran are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are sets of arbitrarily complex conditions, carrying possibly rich information. Most traditional values can be treated as behaviors, and when images are thus treated, they become animations. Although these notions are captured as data types rather than a programming language, we provide them with a denotational semantics, including a proper treatment of real time, to guide reasoning and implementation. A method to effectively and efficiently perform event detection using interval analysis is also described, which relies on the partial information structure on the domain of event times. Fran has been implemented in Hugs, yielding surprisingly good performance for an interpreter-based system. Several examples are given, including the ability to describe physical phenomena involving gravity, springs, velocity, acceleration, etc. using ordinary differential equations.},
	booktitle = {Proceedings of the second {ACM} {SIGPLAN} international conference on Functional programming},
	publisher = {{ACM}},
	author = {Conal Elliott and Paul Hudak},
	year = {1997},
	pages = {263--273}
},

@book{peyton_jones_haskell_2003,
	title = {Haskell 98 Language and Libraries: the Revised Report},
	author = {Simon Peyton Jones},
	year = {2003},
	keywords = {file-import-08-05-29}
},

@article{day_logical_1999,
	title = {Logical abstractions in Haskell},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.2140},
	doi = {10.1.1.37.2140},
	journal = {In Proceedings of the 1999 Haskell Workshop},
	author = {Nancy A Day and John Launchbury and Je Lewis},
	year = {1999}
},

@inproceedings{voigtlnder_bidirectionalization_2009,
	address = {Savannah, {GA,} {USA}},
	title = {Bidirectionalization for free! {(Pearl)}},
	isbn = {978-1-60558-379-2},
	url = {http://portal.acm.org/citation.cfm?id=1480881.1480904&coll=portal&dl=ACM.&type=series&idx=SERIES317&part=series&WantType=Proceedings&title=POPL},
	doi = {10.1145/1480881.1480904},
	abstract = {A bidirectional transformation consists of a function get that takes a source (document or value) to a view and a function put that takes an updated view and the original source back to an updated source, governed by certain consistency conditions relating the two functions. Both the database and programming language communities have studied techniques that essentially allow a user to specify only one of get and put and have the other inferred automatically. All approaches so far to this bidirectionalization task have been syntactic in nature, either proposing a domain-specific language with limited expressiveness but built-in (and composable) backward components, or restricting get to a simple syntactic form from which some algorithm can synthesize an appropriate definition for put. Here we present a semantic approach instead. The idea is to take a general-purpose language, Haskell, and write a higher-order function that takes (polymorphic) get-functions as arguments and returns appropriate put-functions. All this on the level of semantic values, without being willing, or even able, to inspect the definition of get, and thus liberated from syntactic restraints. Our solution is inspired by relational parametricity and uses free theorems for proving the consistency conditions. It works beautifully.},
	booktitle = {Proceedings of the 36th annual {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Janis Voigtl\"{a}nder},
	year = {2009},
	keywords = {bidirectionalization,free theorems,generic programming,program transformation,relational parametricity,view-update problem},
	pages = {165--176}
},

@article{swierstra_combinator_2000,
	title = {Combinator parsers: From toys to tools.},
	volume = {41},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.7601},
	doi = {10.1.1.30.7601},
	number = {1},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {S. Doaitse Swierstra},
	year = {2000}
},

@inbook{frank_one_1999,
	title = {One Step up the Abstraction Ladder: Combining Algebras - From Functional Pieces to a Whole},
	url = {http://dx.doi.org/10.1007/3-540-48384-5_7},
	abstract = {A fundamental scientific question today is how to construct complex systems from simple parts. Science today seems mostly to analyze limited pieces of the puzzle; the combination of these pieces to form a whole is left for later or others. The lack of efficient methods to deal with the combination problem is likely the main reason. How to combine individual results is a dominant question in cognitive science or geography, where phenomena are studied from individuals and at different scales, but the results cannot be brought together. This paper proposes to use parameterized algebras much the same way that we use functional abstraction (procedures in programming languages) to create abstract building blocks which can be combined later. Algebras group operations (which are functional abstractions) and can be combined to construct more complex algebras. Algebras operate therefore at a higher level of abstraction. A table shows the parallels between procedural abstraction and the abstraction by parameterized algebras. This paper shows how algebras can be combined to form more complex pieces and compares the steps to the combination of procedures in programming. The novel contribution is to parameterize algebras and make them thus ready for reuse. The method is first explained with the familiar construction of vector space and then applied to a larger example, namely the description of geometric operations for {GIS,} as proposed in the current draft standard document {ISO} 15046 Part 7: Spatial Schema. It is shown how operations can be grouped, reused, and combined, and useful larger systems built from the pieces. The paper compares the method to combine algebras \^{a}?? which are independent of an implementation \^{a}?? with the current use of object-orientation in programming languages (and in the {UML} notation often used for specification). The widely used\^{a}?? structural\^{a}?? (or subset) polymorphism is justified by implementation considerations, but not appropriate for theory development and abstract specifications for standardization. Parametric polymorphism used for algebras avoids the contravariance of function types (which its semantically confusing consequences). Algebraic methods relate cleanly to the mathematical category theory and the method translates directly to modern functional programming or Java.},
	booktitle = {Spatial Information Theory. Cognitive and Computational Foundations of Geographic Information Science},
	author = {Andrew Frank},
	year = {1999},
	keywords = {aop},
	pages = {751}
},

@article{parker_detection_1983,
	title = {Detection of Mutual Inconsistency in Distributed Systems},
	volume = {{SE-9}},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1703051},
	abstract = {Many distributed systems are now being developed to provide users with convenient access to data via some kind of communications network. In many cases it is desirable to keep the system functioning even when it is partitioned by network failures. A serious problem in this context is how one can support redundant copies of resources such as files (for the sake of reliability) while simultaneously monitoring their mutual consistency (the equality of multiple copies). This is difficult since network faiures can lead to inconsistency, and disrupt attempts at maintaining consistency. In fact, even the detection of inconsistent copies is a nontrivial problem. Naive methods either 1) compare the multiple copies entirely or 2) perform simple tests which will diagnose some consistent copies as inconsistent. Here a new approach, involving version vectors and origin points, is presented and shown to detect single file, multiple copy mutual inconsistency effectively. The approach has been used in the design of {LOCUS,} a local network operating system at {UCLA.}},
	number = {3},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {{DS} Parker and {GJ} Popek and G Rudisin and A Stoughton and {BJ} Walker and E Walton and {JM} Chow and D Edwards and S Kiser and C Kline},
	year = {1983},
	keywords = {vc},
	pages = {240--247}
},

@article{cardelli_understanding_1985,
	title = {On understanding types, data abstraction, and polymorphism},
	volume = {17},
	issn = {0360-0300},
	url = {http://dx.doi.org/10.1145/6041.6042},
	number = {4},
	journal = {{ACM} Comput. Surv.},
	author = {Luca Cardelli and Peter Wegner},
	month = dec,
	year = {1985},
	pages = {471--523}
},

@misc{schrijvers_towards_2007,
	title = {Towards open type functions for Haskell},
	abstract = {We report on an extension of Haskell with type(-level) functions and equality constraints. We illustrate their usefulness in the context of phantom types, {GADTs} and type classes. Problems in the context of type checking are identified and we sketch our solution: a decidable type checking algorithm for a restricted class of type functions. Moreover, functional dependencies are now obsolete: we show how they can be encoded as type functions.},
	journal = {Implementing Functional Languages},
	author = {Tom Schrijvers and Martin Sulzmann and Simon Peyton Jones and Manuel Chakravarty},
	year = {2007},
	keywords = {open,typeclass}
},

@inbook{sowrirajan_managingevolution_2003,
	title = {Managing the Evolution of Distributed and Interrelated Components},
	url = {http://www.springerlink.com/content/pkjprnpx6ffk30ud },
	abstract = {Software systems are increasingly being built by integrating pre-existing components developed by different, geographically distributed organizations. Each component typically evolves independently over time, not only in terms of its functionality, but also in terms of its exposed interfaces and dependencies on other components. Given that those other components may also evolve, creating an application by assembling sets of components typically involves managing a complex web of evolving dependencies. Traditional configuration management systems assume a form of centralized control that simply does not suffice in these situations. Needed are new configuration management systems that span multiple organizations, operate in a distributed and decentralized fashion, and help in managing the consistent evolution of independently developed, inter-related sets of components. A critical aspect of these new configuration management systems is that they must respect the different levels of autonomy, privacy, and trust that exist among different organizations. In this paper, we introduce {TWICS,} an early example of such a new configuration management system. Key aspects of {TWICS} are that it maintains traditional configuration management functionality to support the development of individual components, but integrates policy-driven deployment functionality to support different organizations in evolving their inter-related components.},
	booktitle = {Software Con?guration Management},
	author = {Sundararajan Sowrirajan and Andr\~{A}{\textcopyright} Hoek},
	year = {2003},
	keywords = {evolution},
	pages = {217--230}
},

@phdthesis{lattner_llvm:infrastructure_2002,
	title = {{{LLVM:} An Infrastructure for {Multi-Stage} Optimization}},
	url = {http://llvm.org/pubs/2002-12-LattnerMSThesis.pdf},
	author = {Chris Lattner},
	year = {2002},
	keywords = {bibtex-import,llvm},
	comment = {(private-note){{\textbackslash}em See {{\textbackslash}tt http://llvm.cs.uiuc.edu}.}}
},

@book{okasaki_purely_1999,
	title = {Purely Functional Data Structures},
	isbn = {0521663504},
	publisher = {Cambridge University Press},
	author = {Chris Okasaki},
	month = jul,
	year = {1999},
	pages = {220}
},

@inproceedings{bernardy_comparison_2008,
	address = {Victoria, {BC,} Canada},
	title = {A comparison of c++ concepts and haskell type classes},
	isbn = {978-1-60558-060-9},
	url = {http://dx.doi.org/10.1145/1411318.1411324},
	abstract = {Earlier studies have introduced a list of high-level evaluation criteria to assess how well a language supports generic programming. Since each language that meets all criteria is considered generic, those criteria are not fine-grained enough to differentiate between languages for generic programming. We refine these criteria into a taxonomy that captures differences between type classes in Haskell and concepts in C++, and discuss which differences are incidental and which ones are due to other language features. The taxonomy allows for an improved understanding of language support for generic programming, and the comparison is useful for the ongoing discussions among language designers and users of both languages.},
	booktitle = {{WGP} '08: Proceedings of the {ACM} {SIGPLAN} workshop on Generic programming},
	publisher = {{ACM}},
	author = {Jean Bernardy and Patrik Jansson and Marcin Zalewski and Sibylle Schupp and Andreas Priesnitz},
	year = {2008},
	keywords = {self},
	pages = {37--48}
},

@inproceedings{stewart_dynamic_2005,
	title = {Dynamic applications from the ground up},
	isbn = {{159593071X}},
	url = {http://dx.doi.org/10.1145/1088348.1088352},
	booktitle = {Haskell '05: Proceedings of the 2005 {ACM} {SIGPLAN} workshop on Haskell},
	publisher = {{ACM} Press},
	author = {Don Stewart and Manuel Chakravarty},
	year = {2005},
	keywords = {haskell},
	pages = {27--38}
},

@misc{_scram.pdf_????,
	title = {scram.pdf},
	url = {http://www.cs.umd.edu/~mwh/papers/scram.pdf}
},

@misc{glynn_type_2000,
	title = {Type classes and constraint handling rules},
	url = {http://citeseer.ist.psu.edu/334241.html},
	abstract = {Type classes give a clean approach to dene overloading in programming languages such as Haskell. Haskell 98 supports only single-parameter and constructor type classes. Other extensions such as multi-parameter type classes are highly desired but are still not ocially supported in Haskell 98. Subtle issues arise which possibly might lead to a loss of feasible type inference and ambiguous programs. A proper logical fundament for type class systems seems to be missing where such issues can...},
	author = {K Glynn and P Stuckey and M Sulzmann},
	year = {2000},
	keywords = {typeclass}
},

@article{bahlke_psg_1986,
	title = {The {PSG} system: from formal language definitions to interactive programming environments},
	volume = {8},
	url = {http://portal.acm.org/citation.cfm?id=6465.20890},
	doi = {10.1145/6465.20890},
	abstract = {The {PSG} programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. All language-dependent parts of the environment are generated from an entirely nonprocedural specification of the language's syntax, context conditions, and dynamic semantics. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. {PSG} editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. Program fragments are compiled to terms of the functional language which are executed by an interpreter. The {PSG} generator has been used to produce environments for Pascal, {ALGOL} 60, {MODULA-2,} and the formal language definition language itself.},
	number = {4},
	journal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Rolf Bahlke and Gregor Snelting},
	year = {1986},
	pages = {547--576}
},

@inproceedings{jones_system_1993,
	title = {A system of constructor classes: overloading and implicit higher-order polymorphism},
	url = {http://citeseer.ist.psu.edu/jones95system.html},
	abstract = {This paper describes a flexible type system which combines overloading and higher-order polymorphism in an implicitly typed language using a system of constructor classes -- a natural generalization of type classes in Haskell. We present a wide range of examples which demonstrate the usefulness of such a system. In particular, we show how constructor classes can be used to support the use of monads in a functional language. The underlying type system permits higher-order polymorphism but...},
	booktitle = {{FPCA} '93: Conference on Functional Programming and Computer Architecture, Copenhagen, Denmark},
	publisher = {{ACM} Press},
	author = {Mark Jones},
	year = {1993},
	keywords = {typeclass},
	pages = {52--61}
},

@inbook{bove_formalising_2006,
	title = {Formalising Bitonic Sort in Type Theory},
	url = {http://dx.doi.org/10.1007/11617990_6},
	abstract = {We discuss two complete formalisations of bitonic sort in constructive type theory. Bitonic sort is one of the fastest sorting
algorithms where the sequence of comparisons is not data-dependent. In addition, it is a general recursive algorithm. In the
formalisation we face two main problems: only structural recursion is allowed in type theory, and a formal proof of the correctness
of the algorithm needs to consider quite a number of cases. In our first formalisation we define bitonic sort over dependently-typed
binary trees with information in the leaves and we make use of the 0-1-principle to prove that the algorithm sorts inputs
of arbitrary types. In our second formalisation we use notions from linear orders, lattice theory and monoids. The correctness
proof is directly performed for any ordered set and not only for Boolean values.
},
	booktitle = {Types for Proofs and Programs},
	author = {Ana Bove and Thierry Coquand},
	year = {2006},
	pages = {82--97}
},

@inproceedings{lippe_operation-based_1992,
	title = {Operation-based merging},
	isbn = {0163-5948},
	url = {http://dx.doi.org/10.1145/142868.143753},
	booktitle = {{SDE} 5: Proceedings of the fifth {ACM} {SIGSOFT} symposium on Software development environments},
	publisher = {{ACM} Press},
	author = {Ernst Lippe and Norbert van Oosterom},
	year = {1992},
	keywords = {change-based,good,vc,vc-project},
	pages = {78--87},
	comment = {== introduces changed-based {VC} == conflict-free = commutation}
},

@inproceedings{gibbons_essence_2006,
	title = {The Essence of the Iterator Pattern},
	url = {http://www.comlab.ox.ac.uk/jeremy.gibbons/publications/iterator-msfp.pdf},
	abstract = {The Iterator pattern gives a clean interface for element-by-element access to a collection. Imperative iterations using the pattern have two simultaneous aspects: mapping and accumulating. Various functional iterations model one or other of these, but not both simultaneously. We argue that {McBride} and Paterson's idioms, and in particular the corresponding traverse operator, do exactly this, and therefore capture the essence of the Iterator pattern. We present some axioms for traversal, and illustrate with a simple example, the repmin problem.},
	booktitle = {{Mathematically-Structured} Functional Programming, Kuressaare, Estonia},
	author = {Jeremy Gibbons and Bruno Oliveira and Conor Mcbride and Tarmo Uustalu},
	month = jul,
	year = {2006},
	keywords = {aop}
},

@inbook{ratzinger_eq-mine:_2007,
	title = {{EQ-Mine:} Predicting {Short-Term} Defects for Software Evolution},
	url = {http://dx.doi.org/10.1007/978-3-540-71289-3_3},
	abstract = {We use 63 features extracted from sources such as versioning and issue tracking systems to predict defects in short time frames of two months. Our multivariate approach covers aspects of software projects such as size, team structure, process orientation, complexity of existing solution, difficulty of problem, coupling aspects, time constrains, and testing data. We investigate the predictability of several severities of defects in software projects. Are defects with high severity difficult to predict? Are prediction models for defects that are discovered by internal staff similar to models for defects reported from the field? We present both an exact numerical prediction of future defect numbers based on regression models as well as a classification of software components as defect-prone based on the C4.5 decision tree. We create models to accurately predict short-term defects in a study of 5 applications composed of more than 8.000 classes and 700.000 lines of code. The model quality is assessed based on 10-fold cross validation.},
	booktitle = {Fundamental Approaches to Software Engineering},
	author = {Jacek Ratzinger and Martin Pinzger and Harald Gall},
	year = {2007},
	keywords = {evolution},
	pages = {12--26}
},

@inproceedings{johann_foundations_2008,
	title = {Foundations for structured programming with {GADTs}},
	isbn = {9781595936899},
	url = {http://dx.doi.org/10.1145/1328438.1328475},
	booktitle = {{POPL} '08: Proceedings of the 35th annual {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Patricia Johann and Neil Ghani},
	year = {2008},
	keywords = {aop},
	pages = {297--308}
},

@article{celentano_incremental_1978,
	title = {Incremental {LR} parsers},
	volume = {10},
	number = {4},
	journal = {Acta Informatica},
	author = {A. Celentano},
	year = {1978},
	pages = {307--321}
},

@article{sheard_template_2002,
	title = {Template meta-programming for Haskell},
	volume = {37},
	issn = {0362-1340},
	url = {http://dx.doi.org/10.1145/636517.636528},
	number = {12},
	journal = {{SIGPLAN} Not.},
	author = {Tim Sheard and Simon Peyton Jones},
	month = dec,
	year = {2002},
	keywords = {haskell,template},
	pages = {60--75}
},

@inproceedings{kiselyov_strongly_2004,
	title = {{{Strongly} typed heterogeneous collections}},
	url = {http://dx.doi.org/http://doi.acm.org/10.1145/1017472.1017488},
	booktitle = {{{Haskell} '04: Proceedings of the {ACM} {SIGPLAN} workshop on Haskell}},
	publisher = {{ACM} Press},
	author = {Oleg Kiselyov and Ralf L{\~{A}?}mmel and Keean Schupke},
	year = {2004},
	keywords = {hlist},
	pages = {96--107}
},

@article{zeller_unified_1997,
	title = {Unified versioning through feature logic},
	volume = {6},
	url = {http://citeseer.ist.psu.edu/188532.html},
	abstract = {Software Configuration Management {(SCM)} suffers from tight coupling between {SCM} versioning models and the imposed {SCM} processes. In order to adapt {SCM} tools to {SCM} processes, rather than vice versa, we propose a unified versioning model, the version set model. Version sets denote versions, components, and configurations by feature terms, that is, boolean terms over (feature: value)-attributions. Through feature logic, we deduce consistency of abstract configurations as well as features of...},
	number = {4},
	journal = {{ACM} Transactions on Software Engineering and Methodology},
	author = {Andreas Zeller and Gregor Snelting},
	year = {1997},
	keywords = {vc},
	pages = {398--441}
},

@inproceedings{niu_category-theoretic_2005,
	title = {A category-theoretic approach to syntactic software merging},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1510116},
	abstract = {Software merging is a common and essential activity during the lifespan of large-scale software systems. Traditional textual merge techniques are inadequate for detecting syntactic merge conflicts. In this paper, we propose a domain-independent approach for syntactic software merging that exploits the graph-based structure(s) of programs. We use morphisms between fuzzy graphs to capture the relationships between the structural elements of the programs to be merged, and apply a truth ordering lattice to express inconsistencies and evolutionary properties as we compute the merge. We demonstrate the approach with a three-way consolidation merge in a commercial software system; in particular, we show how analyzing merged call structures can help developers gain a better understanding and control of software evolution.},
	booktitle = {Software Maintenance, 2005. {ICSM'05.} Proceedings of the 21st {IEEE} International Conference on},
	author = {N Niu and S Easterbrook and M Sabetzadeh},
	year = {2005},
	keywords = {vc},
	pages = {197--206}
},

@inproceedings{lmmel_towards_2002,
	address = {Pittsburgh, Pennsylvania},
	title = {Towards generic refactoring},
	isbn = {1-58113-606-4},
	url = {http://portal.acm.org/citation.cfm?doid=570186.570188},
	doi = {10.1145/570186.570188},
	abstract = {We define a challenging and meaningful benchmark for genericity in language processing, namely the notion of generic program refactoring. We provide the first implementation of the benchmark based on functional strategic programming in Haskell. We use the basic refactoring of abstraction extraction as the running example. Our implementation comes as a functional programming framework with hot spots for the language-specific ingredients for refactoring, e.g., means for abstraction construction and destruction, and recognisers for name analysis. The language-parametric framework can be instantiated for various, rather different languages, e.g., Java, Prolog, Haskell, or {XML} schema.},
	booktitle = {Proceedings of the 2002 {ACM} {SIGPLAN} workshop on Rule-based programming},
	publisher = {{ACM}},
	author = {Ralf L\"{a}mmel},
	year = {2002},
	keywords = {frameworks,functional programming,generic programming,language,program transformation,refactoring,reuse,strafunski},
	pages = {15--28}
},

@inproceedings{mcbride_clowns_2008,
	address = {San Francisco, California, {USA}},
	title = {Clowns to the left of me, jokers to the right (pearl): dissecting data structures},
	isbn = {978-1-59593-689-9},
	url = {http://portal.acm.org/citation.cfm?id=1328474&dl=GUIDE&coll=GUIDE&CFID=17840475&CFTOKEN=88160233},
	doi = {10.1145/1328438.1328474},
	abstract = {This paper introduces a small but useful generalisation to the 'derivative' operation on datatypes underlying Huet's notion of 'zipper', giving a concrete representation to one-hole contexts in data which is undergoing transformation. This operator, 'dissection', turns a container-like functor into a bifunctor representing a one-hole context in which elements to the left of the hole are distinguished in type from elements to its right.},
	booktitle = {Proceedings of the 35th annual {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM}},
	author = {Conor {McBride}},
	year = {2008},
	keywords = {datatype,differentiation,dissection,division,generic programming,iteration,stack,tail recursion,traversal,zipper},
	pages = {287--295}
},

@article{plotkin_call-by-name_1975,
	title = {Call-by-name, call-by-value and the ?-calculus},
	volume = {1},
	number = {2},
	journal = {Theoretical Computer Science},
	author = {G. D. Plotkin},
	year = {1975},
	pages = {125--159}
},

@inproceedings{stewart_xmonad_2007,
	title = {Xmonad},
	isbn = {9781595936745},
	url = {http://dx.doi.org/10.1145/1291201.1291218},
	booktitle = {Haskell '07: Proceedings of the {ACM} {SIGPLAN} workshop on Haskell workshop},
	publisher = {{ACM}},
	author = {Don Stewart and Spencer Sjanssen},
	year = {2007},
	keywords = {haskell},
	pages = {119}
},

@inproceedings{gill_short_1993,
	address = {Copenhagen, Denmark},
	title = {A short cut to deforestation},
	isbn = {{0-89791-595-X}},
	url = {http://portal.acm.org/citation.cfm?id=165180.165214},
	doi = {10.1145/165180.165214},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the conference on Functional programming languages and computer architecture},
	publisher = {{ACM}},
	author = {Andrew Gill and John Launchbury and Simon L. Peyton Jones},
	year = {1993},
	pages = {223--232}
},

@article{chakravarty_associated_2005-1,
	title = {Associated type synonyms},
	volume = {40},
	issn = {0362-1340},
	url = {http://dx.doi.org/10.1145/1090189.1086397},
	number = {9},
	journal = {{SIGPLAN} Not.},
	author = {Manuel Chakravarty and Gabriele Keller and Simon Peyton Jones},
	year = {2005},
	keywords = {typeclass},
	pages = {241--253}
},

@inproceedings{villavicencio_reverse_2001,
	title = {Reverse program calculation supported by code slicing},
	url = {http://dx.doi.org/10.1109/WCRE.2001.957808},
	abstract = {This paper sketches a discipline for reverse engineering which combines formal and semi-formal methods. Among the former is the "algebra of programming", which we apply in "reverse order" so as to reconstruct formal specifications of legacy code. The latter includes code slicing, used as a means of trimming down the complexity of handling the formal semantics of all program variables at the same time. A strong point of the approach is its constructive style. Reverse calculations go as far as imploding auxiliary variables, introducing mutual recursion (if applicable) and transforming semantic functions into standard generic programming schemata such as cata/paramorphisms. We illustrate the approach by reversing a piece of code (from C to Haskell) already studied in the code-slicing literature: the word-count (wc) program},
	booktitle = {Reverse Engineering, 2001. Proceedings. Eighth Working Conference on},
	author = {G Villavicencio and {JN} Oliveira},
	year = {2001},
	keywords = {aop},
	pages = {35--45}
},

@article{mens_detecting_2005,
	title = {Detecting Structural Refactoring Conflicts Using Critical Pair Analysis},
	volume = {127},
	url = {http://dx.doi.org/10.1016/j.entcs.2004.08.038},
	abstract = {Refactorings are program transformations that improve the software structure while preserving the external behaviour. In spite of this very useful property, refactorings can still give rise to structural conflicts when parallel evolutions to the same software are made by different developers. This paper explores this problem of structural evolution conflicts in a formal way by using graph transformation and critical pair analysis. Based on experiments carried out in the graph transformation tool {AGG,} we show how this formalism can be exploited to detect and resolve refactoring conflicts.},
	number = {3},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Tom Mens and Gabriele Taentzer and Olga Runge},
	month = apr,
	year = {2005},
	keywords = {vc},
	pages = {113--128}
},

@inproceedings{hughes_polish_2003,
	address = {Uppsala, Sweden},
	title = {Polish parsers, step by step},
	isbn = {1-58113-756-7},
	url = {http://portal.acm.org/citation.cfm?id=944705.944727&coll=ACM&dl=ACM&CFID=13026192&CFTOKEN=34892732},
	doi = {10.1145/944705.944727},
	abstract = {We present the derivation of a space efficient parser combinator library: the constructed parsers do not keep unnecessary references to the input, produce online results and efficiently handle ambiguous grammars. The underlying techniques can be applied in many contexts where traditionally backtracking is {used.We} present two data types, one for keeping track of the progress of the search process, and one for representing the final result in a linear way. Once these data types are combined into a single type, we can perform a breadth-first search, while returning parts of the result as early as possible.},
	booktitle = {Proceedings of the eighth {ACM} {SIGPLAN} international conference on Functional programming},
	publisher = {{ACM}},
	author = {R. John M. Hughes and S. Doaitse Swierstra},
	year = {2003},
	keywords = {ambiguous grammars,breadth-first search,glr parsing,online results,parser combinators,polish representation},
	pages = {239--248}
},

@book{hinze_typed_2006,
	title = {Typed Contracts for Functional Programming},
	volume = {3945},
	url = {http://dx.doi.org/10.1007/11737414_15},
	abstract = {A robust software component fulfills a contract: it expects data satisfying a certain property and promises to return data satisfying another property. The object-oriented community uses the design-by-contract approach extensively. Proposals for language extensions that add contracts to higher-order functional programming have appeared recently. In this paper we propose an embedded domain-specific language for typed, higher-order and first-class contracts, which is both more expressive than previous proposals, and allows for a more informative blame assignment. We take some first steps towards an algebra of contracts, and we show how to define a generic contract combinator for arbitrary algebraic data types. The contract language is implemented as a library in Haskell using the concept of generalised algebraic data types.},
	author = {Ralf Hinze and Johan Jeuring and Andres L\~{A}{\textparagraph}h},
	year = {2006},
	keywords = {blame,contract,invariant},
	pages = {208--225}
},

@inproceedings{greenwald_agreeing_2006,
	title = {Agreeing to Agree: {{C}onflict} Resolution for Optimistically Replicated Data},
	booktitle = {International Symposium on Distributed Computing {(DISC)}},
	author = {Michael Greenwald and Sanjeev Khanna and Keshav Kunal and Benjamin Pierce and Alan Schmitt and Shlomi Dolev},
	year = {2006},
	keywords = {bibtex-import,vc}
},

@article{mckinna_type-correct_2006,
	title = {A type-correct, stack-safe, provably correct, expression compiler},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.4086},
	doi = {10.1.1.105.4086},
	journal = {in Epigram. Submitted to the Journal of Functional Programming},
	author = {James Mckinna and Joel Wright},
	year = {2006}
},

@phdthesis{peng_programmable_????,
	type = {{PhD} Thesis},
	title = {{PROGRAMMABLE} {CONCURRENCY} {IN} A {PURE} {AND} {LAZY}
{LANGUAGE}},
	url = {http://www.cis.upenn.edu/~lipeng/homepage/papers/dissertation.pdf},
	school = {University of Pennsylvania},
	author = {Li Peng}
},

@article{conradi_version_1998,
	title = {Version models for software configuration management},
	volume = {30},
	issn = {0360-0300},
	url = {http://dx.doi.org/10.1145/280277.280280},
	number = {2},
	journal = {{ACM} Comput. Surv.},
	author = {Reidar Conradi and Bernhard Westfechtel},
	month = jun,
	year = {1998},
	keywords = {vc},
	pages = {232--282}
},

@inproceedings{baars_typed_2009,
	address = {New York, {NY,} {USA}},
	title = {Typed Transformations of Typed Abstract Syntax},
	booktitle = {{TLDI} '09: fourth {ACM} {SIGPLAN} Workshop on Types in Language Design and Implementation},
	author = {Arthur Baars and Doaitse Swierstra and Marcos Viera},
	year = {2009}
},

@article{bird_generalised_1999,
	title = {Generalised folds for nested datatypes},
	volume = {11},
	url = {http://dx.doi.org/10.1007/s001650050047},
	abstract = {Abstract.   Nested datatypes generalise regular datatypes in much the same way that context-free languages generalise regular ones. Although the categorical semantics of nested types turns out to be similar to the regular case, the fold functions are more limited because they can only describe natural transformations. Practical considerations therefore dictate the introduction of a generalised fold function in which this limitation can be overcome. In the paper we show how to construct generalised folds systematically for each nested datatype, and show that they possess a uniqueness property analogous to that of ordinary folds. As a consequence, generalised folds satisfy fusion properties similar to those developed for regular datatypes. Such properties form the core of an effective calculational theory of inductive datatypes.},
	number = {2},
	journal = {Formal Aspects of Computing},
	author = {Richard Bird and Ross Paterson},
	year = {1999},
	keywords = {aop},
	pages = {200--222}
},

@misc{czerwinski_polynominal_2007,
	title = {A Polynominal Time Algorithm for Graph Isomorphism},
	url = {http://arxiv.org/abs/0711.2010},
	abstract = {Algorithms testing two graphs for isomorphism known as yet have exponential worst case complexity. In this paper we propose a new algorithm that has polynomial complexity and constructively supplies the evidence that the graph isomorphism lies in P.},
	author = {Reiner Czerwinski},
	year = {2007},
	keywords = {algorithm,graph,isomorphism,polynomial},
	howpublished = {http://arxiv.org/abs/0711.2010}
},

@techreport{rountev_practical_2000,
	title = {Practical points-to analysis for programs built with libraries},
	url = {#},
	author = {A Rountev and B Ryder},
	year = {2000},
	keywords = {points-to}
},

@inproceedings{swierstra_combinator_2009,
	address = {Piriapolis},
	series = {{{LNCS}}},
	title = {Combinator Parsing: A Short Tutorial},
	volume = {5520},
	url = {http://www.cs.uu.nl/research/techreps/repo/CS-2008/2008-044.pdf},
	booktitle = {Language Engineering and Rigorous Software Development},
	publisher = {Springer},
	author = {S. Doaitse Swierstra},
	year = {2009},
	pages = {252--300}
},

@inbook{wallace_partial_2008,
	series = {{LNCS}},
	title = {Partial Parsing: Combining Choice with Commitment},
	volume = {5083/2008},
	url = {http://dx.doi.org/10.1007/978-3-540-85373-2_6},
	abstract = {Parser combinators, often monadic, are a venerable and widely-used solution to read data from some external format. However,
the capability to return a partial parse has, until now, been largely missing. When only a small portion of the entire data
is desired, it has been necessary either to parse the entire input in any case, or to break up the grammar into smaller pieces
and move some work outside the world of combinators.

This paper presents a technique for mixing lazy, demand-driven, parsing with strict parsing, all within the same set of combinators.
The grammar specification remains complete and unbroken, yet only sufficient input is consumed to satisfy the result demanded.
It is built on a combination of applicative and monadic parsers. Monadic parsing alone is insufficient to allow a choice operator to coexist with the early commitment needed for
lazy results. Applicative parsing alone can give partial results, but does not permit context-sensitive grammars. But used
together, we gain both partiality and a flexible ease of use.



Performance results demonstrate that partial parsing is often faster and more space-efficient than strict parsing, but never
worse. The trade-off is that partiality has consequences when dealing with ill-formed input.


},
	booktitle = {Implementation and Application of Functional Languages},
	publisher = {Springer Berlin / Heidelberg},
	author = {Malcolm Wallace},
	year = {2008},
	pages = {93--110}
},

@inproceedings{saraiva_functional_2000,
	title = {Functional Incremental Attribute Evaluation},
	isbn = {{3-540-67263-X}},
	url = {http://portal.acm.org/citation.cfm?id=647476.727632&coll=ACM&dl=ACM&CFID=13026192&CFTOKEN=34892732},
	booktitle = {Proceedings of the 9th International Conference on Compiler Construction},
	publisher = {{Springer-Verlag}},
	author = {Jo\~{a}o Saraiva and S. Doaitse Swierstra and Matthijs F. Kuiper},
	year = {2000},
	pages = {279--294}
},

@misc{peyton_jones_bulk_1996,
	title = {Bulk types with class},
	url = {http://citeseer.ist.psu.edu/peytonjones97bulk.html},
	abstract = {Bulk types --- such as lists, bags, sets, finite maps, and priority queues --- are ubiquitous in programming. Yet many languages don't support them well, even though they have received a great deal of attention, especially from the database community. Haskell is currently among the culprits. This paper has two aims: to identify some of the technical difficulties, and to attempt to address them using Haskell's constructor classes. This paper appears in the proceedings of the 1997 Haskell...},
	author = {Simon Peyton Jones},
	year = {1996},
	keywords = {typeclass}
},

@article{morten_rhiger_type-safe_????,
	title = {Type-safe pattern combinators},
	url = {http://www.itu.dk/people/mir/typesafepatterns.pdf},
	journal = {Journal of Functional Programming},
	author = {Morten Rhiger},
	keywords = {open}
},

@inproceedings{bernardy_yi:editor_2008,
	address = {Victoria, {BC,} Canada},
	title = {Yi: an editor in {{Haskell}} for {{Haskell}}},
	isbn = {978-1-60558-064-7},
	url = {http://portal.acm.org/citation.cfm?id=1411286.1411294&coll=GUIDE&dl=GUIDE&CFID=13026192&CFTOKEN=34892732},
	doi = {10.1145/1411286.1411294},
	abstract = {Yi is a text editor written in Haskell and extensible in Haskell. We take advantage of Haskell's expressive power to define embedded {DSLs} that form the foundation of the editor. In turn, these {DSLs} provide a flexible mechanism to create extended versions of the editor. Yi also provides some support for editing Haskell code.},
	booktitle = {Proceedings of the first {ACM} {SIGPLAN} symposium on Haskell},
	publisher = {{ACM}},
	author = {{Jean-Philippe} Bernardy},
	year = {2008},
	keywords = {editor,functional programming},
	pages = {61--62}
},

@inproceedings{jrvi_algorithm_2006,
	title = {Algorithm specialization in generic programming: challenges of constrained generics in C++},
	isbn = {1595933204},
	url = {http://dx.doi.org/10.1145/1133981.1134014},
	booktitle = {{PLDI} '06: Proceedings of the 2006 {ACM} {SIGPLAN} conference on Programming language design and implementation},
	publisher = {{ACM}},
	author = {Jaakko J\"{a}rvi and Douglas Gregor and Jeremiah Willcock and Andrew Lumsdaine and Jeremy Siek},
	year = {2006},
	keywords = {c,concept,typeclass},
	pages = {272--282}
},

@inproceedings{lerner_composing_2002,
	title = {Composing dataflow analyses and transformations},
	volume = {37},
	isbn = {1581134509},
	url = {http://dx.doi.org/10.1145/503272.503298},
	booktitle = {{POPL} '02: Proceedings of the 29th {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM} Press},
	author = {Sorin Lerner and David Grove and Craig Chambers},
	year = {2002},
	keywords = {cool},
	pages = {270--282}
},

@article{malde_calculating_2006,
	title = {Calculating {PSSM} probabilities with lazy dynamic programming},
	volume = {16},
	url = {http://portal.acm.org/citation.cfm?id=1114011},
	abstract = {Position-specific scoring matrices are one way to represent approximate string patterns, which are commonly encountered in the field of bioinformatics. An important problem that arises with their application is calculating the statistical significance of matches. We review the currently most efficient algorithm for this task, and show how it can be implemented in Haskell, taking advantage of the built-in non-strictness of the language. The resulting program turns out to be an instance of dynamic programming, using lists rather the typical dynamic programming matrix.},
	number = {1},
	journal = {J. Funct. Program.},
	author = {Ketil Malde and Robert Giegerich},
	year = {2006},
	keywords = {dynamic programming},
	pages = {75--81}
},

@misc{mens_uniform_????,
	title = {A Uniform Declarative Framework for Automated Software Merging},
	url = {http://citeseer.ist.psu.edu/mens00uniform.html},
	abstract = {We report on a prototype tool that automates the time-consuming and error-prone process of software merging. Our tool is significantly more flexible than existing merge techniques, as it can detect syntactic, structural as well as semantic conflicts. It is implemented as a general framework for software evolution that can be customised to many different domains. Because of this, it can be used to support evolution of any kind of software artifact, independent of the target language or the...},
	author = {Tom Mens and Kim Mens},
	keywords = {evolution,vc}
},

@inproceedings{hughes_design_1995,
	title = {The Design of a Pretty-printing Library},
	isbn = {3-540-59451-5},
	url = {http://portal.acm.org/citation.cfm?id=734154},
	booktitle = {Advanced Functional Programming, First International Spring School on Advanced Functional Programming {Techniques-Tutorial} Text},
	publisher = {{Springer-Verlag}},
	author = {John Hughes},
	year = {1995},
	pages = {53--96}
},

@inproceedings{jones_system_1993-1,
	title = {A system of constructor classes: overloading and implicit higher-order polymorphism},
	isbn = {{089791595X}},
	url = {http://dx.doi.org/10.1145/165180.165190},
	booktitle = {{FPCA} '93: Proceedings of the conference on Functional programming languages and computer architecture},
	publisher = {{ACM} Press},
	author = {Mark Jones},
	year = {1993},
	keywords = {typeclass},
	pages = {52--61}
},

@inproceedings{garcia_comparative_2003,
	title = {A comparative study of language support for generic programming},
	url = {http://dx.doi.org/http://doi.acm.org/10.1145/949305.949317},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} conference on Object-oriented programing, systems, languages, and applications},
	publisher = {{ACM} Press},
	author = {Ronald Garcia and Jaakko Jarvi and Andrew Lumsdaine and Jeremy Siek and Jeremiah Willcock},
	year = {2003},
	keywords = {sibylle,wgp08},
	pages = {115--134}
},

@article{abbott_&part;_2004,
	title = {{\textbackslash}\&part; for Data: Differentiating Data Structures},
	volume = {65},
	url = {http://portal.acm.org/citation.cfm?id=1227145},
	abstract = {This paper and our conference paper {(Abbott,} Altenkirch, Ghani, and {McBride,} 2003b) explain and analyse the notion of the derivative of a data structure as the type of its one-hole contexts based on the central observation made by {McBride} (2001). To make the idea precise we need a generic notion of a data type, which leads to the notion of a container, introduced in {(Abbott,} Altenkirch, and Ghani, 2003a) and investigated extensively in {(Abbott,} 2003). Using containers we can provide a notion of linear map which is the concept missing from {McBride's} first analysis. We verify the usual laws of differential calculus including the chain rule and establish laws for initial algebras and terminal coalgebras.},
	number = {1-2},
	journal = {Fundam. Inf.},
	author = {Michael Abbott and Thorsten Altenkirch and Conor {McBride} and Neil Ghani},
	year = {2004},
	pages = {1--28}
},

@article{degano_efficient_1988,
	title = {Efficient incremental {LR} parsing for syntax-directed editors},
	volume = {10},
	url = {http://portal.acm.org/citation.cfm?doid=44501.214503},
	doi = {10.1145/44501.214503},
	abstract = {A technique for generating parsers which is an extension to {LR} techniques and is based on parsing table splitting, is presented. Then this technique is slightly extended to support incremental syntax analysis. Given a context-free grammar and a set {{\textquotedblleft}IC{\textquotedblright}} of nonterminals devised to be incremental, a set of subtables is generated to drive the analysis of program fragments derivable from nonterminals in {IC.} The proposed technique generates parsing tables which are considerably smaller than the standard ones, even when incrementality is not exploited. Thus, these tables may be stored as arrays permitting faster access and accurate error handling. Furthermore, our tables are suitable for generating syntax-directed editors which provide a full analytic mode. The efficiency of the analytic component of a syntax-directed editor obtained in this way and its easy integration with the generative component stress the advantages of incremental program writing.},
	number = {3},
	journal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Pierpaolo Degano and Stefano Mannucci and Bruno Mojana},
	year = {1988},
	pages = {345--373}
},

@inproceedings{angelov_visual_2005,
	address = {Tallinn, Estonia},
	title = {Visual haskell: a full-featured haskell development environment},
	isbn = {{159593071X}},
	url = {http://dx.doi.org/10.1145/1088348.1088350},
	booktitle = {Haskell '05: Proceedings of the 2005 {ACM} {SIGPLAN} workshop on Haskell},
	publisher = {{ACM}},
	author = {Krasimir Angelov and Simon Marlow},
	year = {2005},
	keywords = {haskell},
	pages = {5--16}
},

@article{hutton_monadic_1998,
	title = {Monadic parsing in Haskell},
	volume = {8},
	number = {04},
	journal = {Journal of Functional Programming},
	author = {G. Hutton and E. Meijer},
	year = {1998},
	pages = {437--444}
},

@techreport{gregor_core_2008,
	title = {Core Concepts for the {\textbackslash}{\textbackslash}cpp0x Standard Library},
	author = {D Gregor and A Lumsdaine},
	year = {2008},
	keywords = {sibylle,wgp08}
},

@inbook{ma_types_1992,
	title = {Types, abstraction, and parametric polymorphism, part 2},
	url = {http://dx.doi.org/10.1007/3-540-55511-0_1},
	abstract = {The concept of relations over sets is generalized to relations over an arbitrary category, and used to investigate the abstraction (or logical-relations) theorem, the identity extension lemma, and parametric polymorphism, for Cartesian-closed-category models of the simply typed lambda calculus and {PL-category} models of the polymorphic typed lambda calculus. Treatments of Kripke relations and of complete relations on domains are included.},
	booktitle = {Mathematical Foundations of Programming Semantics},
	author = {{QingMing} Ma and John Reynolds},
	year = {1992},
	pages = {1--40}
},

@article{xi_guarded_2003,
	title = {Guarded recursive datatype constructors},
	volume = {38},
	url = {http://portal.acm.org/citation.cfm?id=604150&dl=GUIDE&coll=GUIDE&CFID=38830212&CFTOKEN=12075942},
	doi = {10.1145/640128.604150},
	abstract = {We introduce a notion of guarded recursive (g.r.) datatype constructors, generalizing the notion of recursive datatypes in functional programming languages such as {ML} and Haskell. We address both theoretical and practical issues resulted from this generalization. On one hand, we design a type system to formalize the notion of g.r. datatype constructors and then prove the soundness of the type system. On the other hand, we present some significant applications (e.g., implementing objects, implementing staged computation, etc.) of g.r. datatype constructors, arguing that g.r. datatype constructors can have far-reaching consequences in programming. The main contribution of the paper lies in the recognition and then the formalization of a programming notion that is of both theoretical interest and practical use.},
	number = {1},
	journal = {{SIGPLAN} Not.},
	author = {Hongwei Xi and Chiyan Chen and Gang Chen},
	year = {2003},
	keywords = {{GADT,guarded} recursive datatype constructors},
	pages = {224--235},
	comment = {The {GADT} paper}
},

@article{allison_lazy_1992,
	title = {Lazy {Dynamic-Programming} Can Be Eager},
	volume = {43},
	number = {4},
	journal = {Information Processing Letters},
	author = {L. Allison},
	year = {1992},
	pages = {207--212}
},

@inbook{gibbons_datatype-generic_2007,
	title = {{Datatype-Generic} Programming},
	url = {http://dx.doi.org/10.1007/978-3-540-76786-2_1},
	abstract = {Generic programming aims to increase the flexibility of programming languages, by expanding the possibilities for parametrization \^{a}?? ideally, without also expanding the possibilities for uncaught errors. The term means different things to different people: parametric polymorphism, data abstraction, meta-programming, and so on. We use it to mean polytypism, that is, parametrization by the shape of data structures rather than their contents. To avoid confusion with other uses, we have coined the qualified term datatype-generic programming for this purpose. In these lecture notes, we expand on the definition of datatype-generic programming, and present some examples of datatype-generic programs. We also explore the connection with design patterns in object-oriented programming; in particular, we argue that certain design patterns are just higher-order datatype-generic programs.},
	booktitle = {{Datatype-Generic} Programming},
	author = {Jeremy Gibbons},
	year = {2007},
	keywords = {concept,typeclass},
	pages = {1--71}
},

@inbook{sozeau_first-class_2008,
	title = {{First-Class} Type Classes},
	url = {http://dx.doi.org/10.1007/978-3-540-71067-7_23},
	abstract = {Type Classes have met a large success in Haskell and Isabelle, as a solution for sharing notations by overloading and for specifying with abstract structures by quantification on contexts.
However, both systems are limited by second-class implementations of these constructs, and these limitations are only overcomed
by ad-hoc extensions to the respective systems. We propose an embedding of type classes into a dependent type theory that
is first-class and supports some of the most popular extensions right away. The implementation is correspondingly cheap, general
and integrates well inside the system, as we have experimented in Coq. We show how it can be used to help structured programming
and proving by way of examples.
},
	booktitle = {Theorem Proving in Higher Order Logics},
	author = {Matthieu Sozeau and Nicolas Oury},
	year = {2008},
	pages = {278--293}
},

@inbook{sheard_generic_2007,
	title = {Generic Programming in \^{I}{\textcopyright}mega},
	volume = {Volume 4719/2007},
	url = {http://dx.doi.org/10.1007/978-3-540-76786-2_5},
	abstract = {Generic programming is about making programs more adaptable by making them more general. Generic programs often embody non-traditional kinds of abstraction; ordinary programs are obtained from them by suitably instantiating their parameters. In contrast with normal programs, the parameters of a generic program are often quite rich in structure; for example they may be other programs, types or type constructors, class hierarchies, or even programming paradigms.},
	booktitle = {{Datatype-Generic} Programming},
	author = {Tim Sheard},
	year = {2007},
	keywords = {wgp08},
	pages = {258--284}
},

@article{claessen_poor_1999,
	title = {A poor man's concurrency monad},
	volume = {9},
	url = {http://portal.acm.org/citation.cfm?id=968592.968596},
	abstract = {Without adding any primitives to the language, we define a concurrency monad transformer in Haskell. This allows us to add a limited form of concurrency to any existing monad. The atomic actions of the new monad are lifted actions of the underlying monad. Some extra operations, such as fork, to initiate new processes, are provided. We discuss the implementation, and use some examples to illustrate the usefulness of this construction.},
	number = {3},
	journal = {J. Funct. Program.},
	author = {Koen Claessen},
	year = {1999},
	pages = {313--323}
},

@article{wagner_efficient_1998,
	title = {Efficient and Flexible Incremental Parsing},
	volume = {20},
	number = {5},
	journal = {{ACM} Transactions on Programming Languages and Systems},
	author = {Tim A. Wagner and Suzan L. Graham},
	year = {1998},
	pages = {980--1013}
},

@article{garcia_extended_2007,
	title = {An extended comparative study of language support for generic programming},
	volume = {17},
	issn = {0956-7968},
	url = {http://dx.doi.org/10.1017/S0956796806006198},
	number = {2},
	journal = {J. Funct. Program.},
	author = {Ronald Garcia and Jaakko Jarvi and Andrew Lumsdaine and Jeremy Siek and Jeremiah Willcock},
	month = mar,
	year = {2007},
	keywords = {concept,typeclass},
	pages = {145--205}
},

@techreport{rodriguez_generic_????,
	title = {Generic programming with fixed points for mutually recursive datatypes},
	number = {{UU-CS-2008-019}},
	institution = {Utrecht University},
	author = {Alexey Rodriguez and Stefan Holdermans and Andres L\"{o}h and Johan Jeuring},
	keywords = {multirec}
},

@misc{kiselyov_http://okmij.org/ftp/haskell/types.htmlclass-based-dispatch_????,
	title = {{http://okmij.org/ftp/Haskell/types.html\#class-based-dispatch}},
	url = {http://okmij.org/ftp/Haskell/types.html#class-based-dispatch},
	abstract = {This message gives an example of a dynamic type class cast in Haskell. We want to dispatch on a class of a type rather on a type itself. In other words, we would like to simulate {IsInstanceOf.}},
	author = {Oleg Kiselyov},
	keywords = {typeclass},
	howpublished = {{http://okmij.org/ftp/Haskell/types.html\#class-based-dispatch}}
},

@misc{danvy_three_1991,
	title = {Three steps for the {CPS} transformation},
	url = {http://citeseer.ist.psu.edu/294520.html},
	abstract = {Transforming a \#-term into continuation-passing style {(CPS)} might seem mystical at first, but in fact it can be characterized by three separate aspects: . The values of all intermediate applications are given a name. . The evaluation of these applications is sequentialized based on a traversal of their syntax tree. This traversal mimics the reduction strategy. . The resulting term is equipped with a continuation --- a \#-abstraction whose application to intermediate values yields the final ...},
	author = {O Danvy},
	year = {1991},
	keywords = {cps}
},

@inproceedings{siek_essential_2005,
	address = {Chicago, {{IL},} {{USA}}},
	title = {Essential Language Support for Generic Programming},
	url = {http://dx.doi.org/http://doi.acm.org/10.1145/1065010.1065021},
	booktitle = {{{PLDI}} '05: Proceedings of the {{ACM}} {{SIGPLAN}} 2005 conference on Programming language design and implementation},
	publisher = {{{ACM}} Press},
	author = {Jeremy Siek and Andrew Lumsdaine},
	year = {2005},
	keywords = {concept,file-import-08-06-27},
	pages = {73--84}
},

@inproceedings{kiselyov_lightweight_2007,
	title = {Lightweight static resources, for safe embedded and systems programming},
	url = {http://okmij.org/ftp/Haskell/types.html#ls-resources},
	abstract = {It is an established trend to develop low-level code -- embedded software, device drivers, and operating systems -- using high-level languages, especially functional languages with advanced facilities to abstract and generate code. To be reliable and secure, low-level code must correctly manage space, time, and other resources, so special type systems and verification tools arose to regulate resource access statically. However, a general-purpose functional language practical today can provide the same static assurances, also without run-time overhead. We substantiate this claim and promote the trend with two security kernels in the domain of device drivers: * one built around raw pointers, to track and arbitrate the size, alignment, write permission, and other properties of memory areas across indexing and casting; * the other built around a device register, to enforce protocol and timing requirements while reading from the register. Our style is convenient in Haskell thanks to custom kinds and predicates (as type classes); type-level numbers, functions, and records (using functional dependencies); and mixed type- and term-level programming (enabling partial type signatures).},
	booktitle = {Draft Proceedings of Trends in Functional Programming},
	publisher = {Seton Hall University},
	author = {Oleg Kiselyov and {Chung-Chieh} Shan},
	year = {2007},
	keywords = {programming,typelevel}
},

@inbook{frandsen_dynamic_1995,
	title = {Dynamic algorithms for the Dyck languages},
	url = {http://dx.doi.org/10.1007/3-540-60220-8_54},
	abstract = {We study Dynamic Membership problems for the Dyck languages, the class of strings of properly balanced parentheses. We also study the Dynamic Word problem for the free group. We present deterministic algorithms and data structures which maintain a string under replacements of symbols, insertions, and deletions of symbols, and language membership queries. Updates and queries are handled in polylogarithmic time. We also give both Las Vegas- and Monte Carlo-type randomised algorithms to achieve better running times, and present lower bounds on the complexity for variants of the problems.},
	booktitle = {Algorithms and Data Structures},
	author = {Gudmund Frandsen and Thore Husfeldt and Peter Miltersen and Theis Rauhe and S?ren Skyum},
	year = {1995},
	pages = {98--108}
},

@techreport{gregor_proposed_2007-1,
	title = {Proposed {{W}ording} for {{C}oncepts} {({R}evision} 3)},
	author = {D Gregor and B Stroustrup and J Siek and James Widman},
	year = {2007},
	keywords = {sibylle,wgp08}
},

@inproceedings{jrvi_library_2007,
	title = {Library composition and adaptation using {{{\textbackslash}{\textbackslash}Cpp}} concepts},
	booktitle = {Generative Programming and Component Engineering, 6th International Conference, {GPCE} 2007, Salzburg, Austria, October 1-3, 2007, Proceedings},
	author = {J J\"{a}rvi and M Marcus and J Smith and C Consel and J Lawall},
	year = {2007},
	keywords = {sibylle,wgp08},
	pages = {73--82}
},

@article{johnsson_efficient_1998,
	title = {Efficient graph algorithms using lazy monolithic arrays},
	volume = {8},
	url = {http://portal.acm.org/citation.cfm?id=969592.969594},
	abstract = {Many, perhaps even most, algorithms that involve data structures are traditionally expressed by incremental updates of the data structures. In functional languages, however, incremental updates are usually both clumsy and inefficient, especially when the data structure is an {array.In} functional languages, we instead prefer to express such array algorithms using monolithic arrays {\textendash} wholesale creation of the final answer {\textendash} both for succinctness of expression, efficiency (only one array created) and (sometimes) implicit parallelism. The ease with which the solution can be reformulated of course depends on the problem, and varies from trivial (e.g. matrix multiplication), to challenging (e.g. solving linear equation systems using Gauss elimination, which in fact can be done by creating only two arrays, recursively defined, of which one is the answer). Other problems have been notoriously resistant to attack; these usually involve some unpredictable processing order of the elements. One such problem is graph marking, i.e. marking the nodes reachable from a set of roots. Hitherto, no functional method has been known except emulating the traditional imperative solution {(King} \& Launchbury, 1995; Launchbury \& Peyton Jones, {1995).The} contribution of this paper is to show how this problem, and some related ones, can be solved using a novel array creation primitive, lazier than previous ones.},
	number = {4},
	journal = {J. Funct. Program.},
	author = {Thomas Johnsson},
	year = {1998},
	pages = {323--333},
	comment = {{lazyArray} :: {(Int,} Int) -{\textgreater} [(int,a)] -{\textgreater} Array Int [a]}
},

@book{austern_generic_1998,
	title = {Generic programming and the {STL:} using and extending the C++ Standard Template Library},
	isbn = {0201309564},
	url = {http://portal.acm.org/citation.cfm?id=288771},
	publisher = {{Addison-Wesley} Longman Publishing Co., Inc.},
	author = {Matthew Austern},
	year = {1998},
	keywords = {concept}
},

@inproceedings{magnusson_fine-grained_1993,
	title = {{Fine-Grained} Revision Control for Collaborative Software Development},
	url = {http://citeseer.ist.psu.edu/magnusson93finegrained.html},
	abstract = {This paper presents a framework for controlling the evolution of complex software systems concurrently developed by teams of software engineers. A general technique for fine-grained revision control of hierarchically structured information, such as programs and documents, is described and evaluated. All levels in the hierarchy are revision controlled, leaves as well as branch nodes. The technique supports sharing of unchanged nodes among revisions, automatic change propagation, and...},
	booktitle = {Proceedings of {ACM} {{SIGSOFT}} '93: Symposium on Foundations of Software Engineering},
	author = {Boris Magnusson and Ulf Asklund and Sten Min{\textbackslash}"{or}},
	year = {1993},
	keywords = {vc},
	pages = {21--30}
},

@article{robbes_change-based_2007,
	title = {A Change-based Approach to Software Evolution},
	volume = {166},
	url = {http://dx.doi.org/10.1016/j.entcs.2006.06.015},
	abstract = {Software evolution research is limited by the amount of information available to researchers: Current version control tools do not store all the information generated by developers. They do not record every intermediate version of the system issued, but only snapshots taken when a developer commits source code into the repository. Additionally, most software evolution analysis tools are not a part of the day-to-day programming activities, because analysis tools are resource intensive and not integrated in development environments. We propose to model development information as change operations that we retrieve directly from the programming environment the developers are using, while they are effecting changes to the system. This accurate and incremental information opens new ways for both developers and researchers to explore and evolve complex systems.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Romain Robbes and Michele Lanza},
	year = {2007},
	keywords = {evolution},
	pages = {93--109}
},

@misc{foster_harmony:generic_2005,
	title = {Harmony: A Generic Synchronization Framework for Heterogeneous, Replicated Data},
	author = {Nathan Foster},
	year = {2005},
	keywords = {bibtex-import,vc},
	comment = {{(private-note)Poster} at {DB-IR} Day.}
},

@inproceedings{hinze_type-indexed_2002,
	title = {{Type-Indexed} Data Types},
	isbn = {3-540-43857-2},
	url = {http://portal.acm.org/citation.cfm?id=747315},
	booktitle = {Proceedings of the 6th International Conference on Mathematics of Program Construction},
	publisher = {{Springer-Verlag}},
	author = {Ralf Hinze and Johan Jeuring and Andres L\"{o}h},
	year = {2002},
	pages = {148--174}
},

@inproceedings{da_silva_lighthouse:_2006,
	title = {Lighthouse: coordination through emerging design},
	isbn = {1595936211},
	url = {http://dx.doi.org/10.1145/1188835.1188838},
	booktitle = {eclipse '06: Proceedings of the 2006 {OOPSLA} workshop on eclipse technology {eXchange}},
	publisher = {{ACM} Press},
	author = {Isabella da Silva and Ping Chen and Christopher Van der Westhuizen and Roger Ripley and Andr{\textbackslash}'e van der Hoek},
	year = {2006},
	keywords = {conflict-avoidance},
	pages = {11--15}
},

@article{hughes_novel_1986,
	title = {A novel representation of lists and its application to the function "reverse"},
	volume = {22},
	url = {http://portal.acm.org/citation.cfm?id=8468.8475},
	number = {3},
	journal = {Inf. Process. Lett.},
	author = {R J M Hughes},
	year = {1986},
	pages = {141--144}
},

@techreport{becker_working_2007,
	title = {Working Draft, Standard for Programming Language {\textbackslash}{\textbackslash}cpp},
	author = {Pete Becker},
	year = {2007},
	keywords = {sibylle,wgp08}
},

@article{huet_zipper_1997,
	title = {The Zipper},
	volume = {7},
	url = {http://portal.acm.org/citation.cfm?id=969872},
	abstract = {Almost every programmer has faced the problem of representing a tree together with a subtree that is the focus of attention, where that focus may move left, right, up or down the tree. The Zipper is Huet's nifty name for a nifty data structure which fulfills this need. I wish I had known of it when I faced this task, because the solution I came up with was not quite so efficient or elegant as the Zipper.},
	number = {5},
	journal = {J. Funct. Program.},
	author = {G\'{e}rard Huet},
	year = {1997},
	pages = {549--554}
},

@inproceedings{willcock_formalization_2004,
	title = {A Formalization of Concepts for Generic Programming},
	booktitle = {Concepts: a Linguistic Foundation of Generic Programming at Adobe Tech Summit},
	publisher = {{{Adobe} Systems}},
	author = {Jeremiah Willcock and Jaakko J{\~{A}?}rvi and Andrew Lumsdaine and David Musser},
	year = {2004},
	keywords = {sibylle,wgp08}
},

@inproceedings{wadler_to_1989,
	title = {How to make ad-hoc polymorphism less ad hoc},
	isbn = {0897912942},
	url = {http://dx.doi.org/10.1145/75277.75283},
	booktitle = {{POPL} '89: Proceedings of the 16th {ACM} {SIGPLAN-SIGACT} symposium on Principles of programming languages},
	publisher = {{ACM} Press},
	author = {P Wadler and S Blott},
	year = {1989},
	keywords = {typeclass},
	pages = {60--76}
},

@misc{_main_????,
	title = {Main Page - How to be a Programmer (2008)},
	url = {http://www.trickjarrett.com/programmer/index.php/Main_Page}
},

@inproceedings{runciman_smallcheck_2008,
	address = {Victoria, {BC,} Canada},
	title = {Smallcheck and lazy smallcheck: automatic exhaustive testing for small values},
	isbn = {978-1-60558-064-7},
	url = {http://portal.acm.org/citation.cfm?id=1411292},
	doi = {10.1145/1411286.1411292},
	abstract = {This paper describes two Haskell libraries for property-based testing. Following the lead of {QuickCheck,} these testing libraries {SmallCheck} and Lazy {SmallCheck} also use type-based generators to obtain test-sets of finite values for which properties are checked, and report any counter-examples found. But instead of using a sample of randomly generated values they test properties for all values up to some limiting depth, progressively increasing this limit. The paper explains the design and implementation of both libraries and evaluates them in comparison with each other and with {QuickCheck.}},
	booktitle = {Proceedings of the first {ACM} {SIGPLAN} symposium on Haskell},
	publisher = {{ACM}},
	author = {Colin Runciman and Matthew Naylor and Fredrik Lindblad},
	year = {2008},
	keywords = {embedded language,exhaustive search,lazy evaluation,property-based testing,type classes},
	pages = {37--48}
},

@inproceedings{danvy_defunctionalization_2001,
	address = {Florence, Italy},
	title = {Defunctionalization at work},
	isbn = {{1-58113-388-X}},
	url = {http://portal.acm.org/citation.cfm?id=773184.773202},
	doi = {10.1145/773184.773202},
	abstract = {Reynolds's defunctionalization technique is a whole-program transformation from higher-order to first-order functional programs. We study practical applications of this transformation and uncover new connections between seemingly unrelated higher-order and first-order specifications and between their correctness proofs. Defunctionalization therefore appearsboth as a springboard for rev ealing new connections and as a bridge for transferring existing results between the first-order world and the higher-order world.},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} international conference on Principles and practice of declarative programming},
	publisher = {{ACM}},
	author = {Olivier Danvy and Lasse R. Nielsen},
	year = {2001},
	keywords = {church encoding,closure conversion,continuation-passing style (cps),continuations,cps transformation,defunctionalization,direct-style transformation,first-order programs,higher-order programs,lambda-lifting,ml,regular expressions,scheme,supercombinator conversion,syntactic theories},
	pages = {162--174}
},

@inbook{meertens_calculate_1996,
	title = {Calculate polytypically!},
	url = {http://dx.doi.org/10.1007/3-540-61756-6_73},
	abstract = {A polytypic function definition is a function definition that is parametrised with a datatype. It embraces a class of algorithms. As an example we define a simple polytypic crush combinator that can be used to calculate polytypically. The ability to define functions polytypically adds another level of flexibility in the reusability of programming idioms and in the design of libraries of interoperable components.},
	booktitle = {Programming Languages: Implementations, Logics, and Programs},
	author = {Lambert Meertens},
	year = {1996},
	keywords = {aop},
	pages = {1--16}
},

@article{ghezzi_augmenting_1980,
	title = {Augmenting Parsers to Support Incrementality},
	volume = {27},
	number = {3},
	journal = {Journal of the {ACM} {(JACM)}},
	author = {C. Ghezzi and D. Mandrioli},
	year = {1980},
	pages = {564--579}
},

@inproceedings{nielsen_call-by-name_1995,
	title = {{Call-By-Name} {CPS-Translation} as a {Binding-Time} Improvement},
	isbn = {3-540-60360-3},
	url = {http://portal.acm.org/citation.cfm?id=717677},
	booktitle = {Proceedings of the Second International Symposium on Static Analysis},
	publisher = {{Springer-Verlag}},
	author = {Kristian Nielsen and Morten Heine S?rensen},
	year = {1995},
	pages = {296--313}
},

@inproceedings{kim_program_2006,
	title = {Program element matching for multi-version program analyses},
	isbn = {1595933972},
	url = {http://dx.doi.org/10.1145/1137983.1137999},
	booktitle = {{MSR} '06: Proceedings of the 2006 international workshop on Mining software repositories},
	publisher = {{ACM} Press},
	author = {Miryung Kim and David Notkin},
	year = {2006},
	keywords = {evolution,vc},
	pages = {58--64}
},

@article{sulzmann_understanding_2007,
	title = {Understanding functional dependencies via constraint handling rules},
	volume = {17},
	issn = {0956-7968},
	url = {http://dx.doi.org/10.1017/S0956796806006137},
	number = {1},
	journal = {J. Funct. Program.},
	author = {Martin Sulzmann and Gregory Duck and Simon {Peyton-Jones} and Peter Stuckey},
	year = {2007},
	keywords = {typeclass},
	pages = {83--129}
},

@misc{hughes_restricted_1999,
	title = {Restricted Datatypes in Haskell},
	url = {http://citeseer.ist.psu.edu/hughes99restricted.html},
	abstract = {The implementations of abstract type constructors must often restrict the type parameters: for example, one implementation of sets may require equality on the element type, while another implementation requires an ordering. Haskell has no mechanism to abstract over such restrictions, which can hinder us from replacing one implementation by another, or making several implementations instances of the same class. This paper proposes a language extension called restricted data types to...},
	author = {J Hughes},
	year = {1999}
},

@misc{peyton_jones_type_1997,
	title = {Type classes: an exploration of the design space},
	url = {http://citeseer.ist.psu.edu/43480.html},
	abstract = {When type classes were first introduced in Haskell they were regarded as a fairly experimental language feature, and therefore warranted a fairly conservative design. Since that time, practical experience has convinced many programmers of the benefits and convenience of type classes. However, on occasion, these same programmers have discovered examples where seemingly natural applications for type class overloading are prevented by the restrictions imposed by the Haskell design. It is possible...},
	author = {Simon Peyton Jones and Mark Jones and Erik Meijer},
	year = {1997},
	keywords = {typeclass}
},

@phdthesis{carlsson_fudgets:_1998,
	type = {{PhD} Thesis},
	title = {Fudgets: Purely Functional Processes with Applications to Graphical User Interfaces},
	school = {Chalmers tekniska h\"{o}gskola},
	author = {M. Carlsson and T. Hallgren},
	year = {1998}
},

@inproceedings{ramsey_algebraic_2001,
	title = {An algebraic approach to file synchronization},
	isbn = {0163-5948},
	url = {http://dx.doi.org/10.1145/503209.503233},
	booktitle = {{ESEC/FSE-9:} Proceedings of the 8th European software engineering conference held jointly with 9th {ACM} {SIGSOFT} international symposium on Foundations of software engineering},
	publisher = {{ACM} Press},
	author = {Norman Ramsey and El\${\textasciicircum}''\$od Csirmaz},
	year = {2001},
	keywords = {synch,vc},
	pages = {175--185}
},

@article{tichy_rcs_1985,
	title = {{RCS} --- A System for Version Control},
	volume = {15},
	url = {http://citeseer.ist.psu.edu/tichy85rcs.html},
	abstract = {An important problem in program development and maintenance is version control, i.e., the task of keeping a software system consisting of many versions and configurations well organized. The Revision Control System {(RCS)} is a software tool that assists with that task. {RCS} manages revisions of text documents, in particular source programs, documentation, and test data. It automates the storing, retrieval, logging and identification of revisions, and it provides selection mechanisms for composing ...},
	number = {7},
	journal = {Software --- Practice and Experience},
	author = {Walter Tichy},
	year = {1985},
	keywords = {vc},
	pages = {637--654}
},

@inproceedings{sulzmann_system_2007,
	title = {System F with type equality coercions},
	isbn = {{159593393X}},
	url = {http://dx.doi.org/10.1145/1190315.1190324},
	booktitle = {{TLDI} '07: Proceedings of the 2007 {ACM} {SIGPLAN} international workshop on Types in languages design and implementation},
	publisher = {{ACM}},
	author = {Martin Sulzmann and Manuel Chakravarty and Simon Peyton Jones and Kevin Donnelly},
	year = {2007},
	keywords = {typeclass},
	pages = {53--66}
},

@inbook{ruffell_pervasiveness_2006,
	title = {The Pervasiveness of Global Data in Evolving Software Systems},
	url = {http://dx.doi.org/10.1007/11693017_30},
	abstract = {In this research, we investigate the role of common coupling in evolving software systems. It can be argued that most software developers understand that the use of global data has many harmful side-effects, and thus should be avoided. We are therefore interested in the answer to the following question: if global data does exist within a software project, how does global data usage evolve over a software project\^{a}??s lifetime? Perhaps the constant refactoring and perfective maintenance eliminates global data usage, or conversely, perhaps the constant addition of features and rapid development introduce an increasing reliance on global data? We are also interested in identifying if global data usage patterns are useful as a software metric that is indicative of an interesting or significant event in the software\^{a}??s lifetime.},
	booktitle = {Fundamental Approaches to Software Engineering},
	author = {Fraser Ruffell and Jason Selby},
	year = {2006},
	keywords = {coupling,evolution},
	pages = {396--410}
},

@article{voigtlnder_much_2008,
	title = {Much ado about two (pearl): a pearl on parallel prefix computation},
	volume = {43},
	url = {http://portal.acm.org/citation.cfm?id=1328445},
	doi = {10.1145/1328897.1328445},
	abstract = {This pearl develops a statement about parallel prefix computation in the spirit of Knuth's {0-1-Principle} for oblivious sorting algorithms. It turns out that 0-1 is not quite enough here. The perfect hammer for the nails we are going to drive in is relational parametricity.},
	number = {1},
	journal = {{SIGPLAN} Not.},
	author = {Janis Voigtl\"{a}nder},
	year = {2008},
	keywords = {0-1-principle,free theorems,parallel prefix computation,relational parametricity},
	pages = {29--35}
}